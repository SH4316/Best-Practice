# 대규모 언어 모델(LLM) 개발 강의 계획서
## 대학원 석사 과정

### 과목 개요
본 과목은 대규모 언어 모델(LLM) 개발에 필요한 이론적 기초와 실용적 구현 기술을 체계적으로 다루는 대학원 석사 과정 강의입니다. 수강생은 수학적 기초, 머신러닝/딥러닝 기본 개념, 트랜스포머 아키텍처, LLM 훈련 및 미세조정, 평가 및 응용, 연구 방법론까지 전체 파이프라인을 학습하게 됩니다.

### 강의 목표
1. LLM 개발에 필요한 수학적, 프로그래밍적 기초 지식 습득
2. 트랜스포머 아키텍처와 LLM의 핵심 원리 이해
3. LLM 훈련, 미세조정, 평가 방법에 대한 실용적 기술 습득
4. LLM 연구 방법론과 석사 논문 준비를 위한 기반 마련
5. 실제 프로젝트를 통한 LLM 개발 경험 축적

### 강의 형식
- 이론 강의 (50%): 핵심 개념, 원리, 최신 연구 동향
- 실습 세션 (40%): 코드 구현, 프레임워크 사용, 프로젝트 수행
- 세미나 (10%): 논문 발표, 연구 아이디어 공유

### 총 강의 시간: 16주 (주 3시간, 총 48시간)

---

## 주차별 강의 계획

### 1부: 기초 다지기 (1-4주)

#### 1주차: 과정 소개 및 LLM 개요
- **이론 강의**:
  - LLM의 역사와 발전
  - 현대 LLM 생태계 (GPT, BERT, LLaMA 등)
  - 학습 로드맵 소개
- **실습 세션**:
  - 개발 환경 설정 (Python, PyTorch, Jupyter)
  - 기본 LLM API 사용법 (Hugging Face, OpenAI)

#### 2주차: 수학적 기초
- **이론 강의**:
  - 선형대수: 벡터, 행렬 연산, 고유값 분해
  - 미적분학: 도함수, 연쇄 법칙, 최적화 기초
  - 확률과 통계: 확률 분포, 베이즈 정리
- **실습 세션**:
  - NumPy를 이용한 선형대수 연산
  - 미적분학적 개념의 코드 구현

#### 3주차: 머신러닝 기초
- **이론 강의**:
  - 지도 학습과 비지도 학습
  - 회귀와 분류
  - 과적합과 정규화
- **실습 세션**:
  - 간단한 선형 회귀 모델 구현
  - scikit-learn을 이용한 기본 머신러닝 알고리즘 적용

#### 4주차: 딥러닝 기초
- **이론 강의**:
  - 신경망의 기본 구조
  - 역전파 알고리즘
  - 활성화 함수와 손실 함수
- **실습 세션**:
  - PyTorch를 이용한 기본 신경망 구현
  - 간단한 분류 문제 해결

---

### 2부: 딥러닝 심화와 트랜스포머 (5-8주)

#### 5주차: 심화 딥러닝 아키텍처
- **이론 강의**:
  - 합성곱 신경망(CNN)
  - 순환 신경망(RNN)과 LSTM
  - 어텐션 메커니즘의 기초
- **실습 세션**:
  - CNN을 이용한 이미지 분류
  - RNN을 이용한 텍스트 생성

#### 6주차: 트랜스포머 아키텍처 (1)
- **이론 강의**:
  - "Attention Is All You Need" 논문 심층 분석
  - 셀프 어텐션 메커니즘
  - 멀티-헤드 어텐션
- **실습 세션**:
  - 셀프 어텐션 메커니즘 구현
  - 스케일드 닷 프로덕트 어텐션 구현

#### 7주차: 트랜스포머 아키텍처 (2)
- **이론 강의**:
  - 위치 인코딩
  - 인코더-디코더 구조
  - 포지션 와이즈 피드포워드 네트워크
- **실습 세션**:
  - 위치 인코딩 구현
  - 간단한 트랜스포머 블록 구현

#### 8주차: 중간 프로젝트
- **프로젝트**:
  - 간단한 트랜스포머 모델从头 구현
  - 기계 번역 또는 텍스트 분류 문제에 적용

---

### 3부: LLM 훈련과 미세조정 (9-12주)

#### 9주차: LLM 훈련 기초
- **이론 강의**:
  - 토크나이저 (BPE, WordPiece, SentencePiece)
  - 언어 모델링 목적 함수
  - 분산 훈련 기법
- **실습 세션**:
  - 다양한 토크나이저 구현 및 비교
  - 간단한 언어 모델 훈련

#### 10주차: 사전 훈련 전략
- **이론 강의**:
  - 대규모 데이터셋 수집과 전처리
  - 마스크드 언어 모델링과 인과적 언어 모델링
  - 학습률 스케줄링과 최적화 기법
- **실습 세션**:
  - 작은 규모의 데이터셋으로 사전 훈련 시뮬레이션
  - 학습률 스케줄러 구현

#### 11주차: 미세조정 방법론
- **이론 강의**:
  - 지도 미세조정(SFT)
  - 인간 피드백을 통한 강화 학습(RLHF)
  - 매개변수 효율적 미세조정(LoRA, QLoRA 등)
- **실습 세션**:
  - Hugging Face를 이용한 모델 미세조정
  - LoRA 구현 및 적용

#### 12주차: 고급 미세조정 기법
- **이론 강의**:
  - 프롬프트 튜닝과 프리픽스 튜닝
  - 어댑터 레이어
  - 다중 작업 학습
- **실습 세션**:
  - 다양한 미세조정 기법 비교 실험
  - 특정 도메인에 대한 모델 미세조정

---

### 4부: LLM 평가와 응용 (13-15주)

#### 13주차: LLM 평가 방법론
- **이론 강의**:
  - 퍼플렉서티와 내장 평가 지표
  - 과업별 평가 지표 (BLEU, ROUGE 등)
  - 종합 벤치마크 (MMLU, HellaSwag 등)
- **실습 세션**:
  - 다양한 평가 지표 구현
  - 기존 LLM 성능 평가

#### 14주차: LLM 응용 및 시스템 통합
- **이론 강의**:
  - 검색 증강 생성(RAG)
  - 체인 오브 씽킹 프롬프트
  - LLM 에이전트와 도구 사용
- **실습 세션**:
  - RAG 시스템 구현
  - 간단한 LLM 에이전트 개발

#### 15주차: 최신 연구 동향과 특수 주제
- **이론 강의**:
  - 멀티모달 모델
  - 효율적 추론 기법
  - 한국어 LLM의 특수성
- **실습 세션**:
  - 최신 논문 구현 아이디어 발표
  - 최종 프로젝트 계획 발표

---

### 5부: 연구 방법론과 프로젝트 (16주)

#### 16주차: 최종 프로젝트 발표 및 연구 방법론
- **세션**:
  - 최종 프로젝트 발표
  - LLM 연구 방법론
  - 석사 논문 준비 가이드

---

## 평가 방법

- **중간 프로젝트 (25%)**: 간단한 트랜스포머 모델 구현
- **실습 과제 (30%)**: 주별 실습 과제 수행
- **최종 프로젝트 (35%)**: LLM 관련 자유 주제 프로젝트
- **참여도 (10%)**: 수업 참여, 논문 토론, 질의응답

## 필수 참고 자료

### 교재
- "Speech and Language Processing" by Dan Jurafsky and James H. Martin
- "Deep Learning" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville
- "Natural Language Processing" by Jacob Eisenstein

### 필수 논문
- "Attention Is All You Need" (Vaswani et al., 2017)
- "BERT: Pre-training of Deep Bidirectional Transformers" (Devlin et al., 2018)
- "Language Models are Few-Shot Learners" (GPT-3, Brown et al., 2020)
- "Training Language Models to Follow Instructions with Human Feedback" (InstructGPT, Ouyang et al., 2022)

### 온라인 자료
- Stanford CS224N: NLP with Deep Learning
- Stanford CS324: Large Language Models
- Hugging Face Documentation
- PyTorch Documentation

## 개발 환경 요구사항

- Python 3.8+
- PyTorch 1.12+
- Jupyter Notebook
- Hugging Face Transformers
- GPU 권장 (NVIDIA RTX 3060 이상)

## 학습 성공 전략

1. **이론과 실습의 균형**: 이론적 이해와 코드 구현을 병행
2. **점진적 학습**: 기초부터 심화까지 체계적으로 접근
3. **능동적 참여**: 논문 읽기, 코드 분석, 질문하기
4. **프로젝트 중심 학습**: 실제 문제 해결에 집중
5. **동료 학습**: 팀 프로젝트, 코드 리뷰, 아이디어 공유