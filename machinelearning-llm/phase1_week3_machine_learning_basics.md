# 3ì£¼ì°¨: ë¨¸ì‹ ëŸ¬ë‹ ê¸°ì´ˆ

## ê°•ì˜ ëª©í‘œ
- ë¨¸ì‹ ëŸ¬ë‹ì˜ ê¸°ë³¸ ê°œë…ê³¼ ì¢…ë¥˜ ì´í•´
- ì§€ë„ í•™ìŠµê³¼ ë¹„ì§€ë„ í•™ìŠµì˜ ì›ë¦¬ì™€ ì•Œê³ ë¦¬ì¦˜ ìŠµë“
- íšŒê·€ì™€ ë¶„ë¥˜ ë¬¸ì œì˜ í•´ê²° ë°©ë²• í•™ìŠµ
- ê³¼ì í•©ê³¼ ì •ê·œí™”ì˜ ê°œë…ê³¼ í•´ê²°ì±… ì´í•´
- ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í‰ê°€ ë°©ë²• ìŠµë“

## ì´ë¡  ê°•ì˜ (90ë¶„)

### 1. ë¨¸ì‹ ëŸ¬ë‹ ê°œìš” (20ë¶„)

#### ë¨¸ì‹ ëŸ¬ë‹ì˜ ì •ì˜ì™€ ì—­ì‚¬
**ì •ì˜**
- í†° ë¯¸ì²¼ì˜ ì •ì˜: "ê²½í—˜(E)ì„ í†µí•´ íŠ¹ì • ì‘ì—…(T)ì˜ ì„±ëŠ¥(P)ì„ í–¥ìƒì‹œí‚¤ëŠ” ì»´í“¨í„° í”„ë¡œê·¸ë¨"
- ì „í†µì  í”„ë¡œê·¸ë˜ë°ê³¼ì˜ ì°¨ì´: ëª…ì‹œì  ê·œì¹™ ëŒ€ì‹  ë°ì´í„°ì—ì„œ íŒ¨í„´ í•™ìŠµ
- LLMê³¼ì˜ ê´€ê³„: ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì€ ë¨¸ì‹ ëŸ¬ë‹ì˜ íŠ¹ì • ë¶„ì•¼

**ë¨¸ì‹ ëŸ¬ë‹ì˜ ì—­ì‚¬**
- 1950ë…„ëŒ€: í¼ì…‰íŠ¸ë¡ , ì´ˆê¸° ì‹ ê²½ë§
- 1980-90ë…„ëŒ€: ê²°ì • íŠ¸ë¦¬, ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹ , ì•™ìƒë¸” ë°©ë²•
- 2000ë…„ëŒ€: ë”¥ëŸ¬ë‹ì˜ ë¶€ìƒ, ì»¨ë³¼ë£¨ì…˜ ì‹ ê²½ë§
- 2010ë…„ëŒ€-í˜„ì¬: ëŒ€ê·œëª¨ ëª¨ë¸, íŠ¸ëœìŠ¤í¬ë¨¸, LLM

#### ë¨¸ì‹ ëŸ¬ë‹ì˜ ì¢…ë¥˜

**ì§€ë„ í•™ìŠµ(Supervised Learning)**
- ì •ì˜: ë ˆì´ë¸”ëœ ë°ì´í„°ë¡œë¶€í„° ì…ë ¥-ì¶œë ¥ ê´€ê³„ í•™ìŠµ
- íŠ¹ì§•: ì •ë‹µì´ ìˆëŠ” ë°ì´í„°, ëª…ì‹œì  í”¼ë“œë°±
- ì˜ˆì‹œ: ìŠ¤íŒ¸ ë©”ì¼ ë¶„ë¥˜, ì´ë¯¸ì§€ ì¸ì‹, ë²ˆì—­
- LLM ì ìš©: ì§€ë„ ë¯¸ì„¸ì¡°ì •(SFT), ëª…ë ¹ì–´ ë”°ë¥´ê¸° í•™ìŠµ

**ë¹„ì§€ë„ í•™ìŠµ(Unsupervised Learning)**
- ì •ì˜: ë ˆì´ë¸” ì—†ëŠ” ë°ì´í„°ì—ì„œ ìˆ¨ê²¨ëœ êµ¬ì¡° ë°œê²¬
- íŠ¹ì§•: ì •ë‹µì´ ì—†ëŠ” ë°ì´í„°, ì•”ì‹œì  íŒ¨í„´ ë°œê²¬
- ì˜ˆì‹œ: í´ëŸ¬ìŠ¤í„°ë§, ì°¨ì› ì¶•ì†Œ, ì´ìƒ íƒì§€
- LLM ì ìš©: ì‚¬ì „ í›ˆë ¨, í‘œí˜„ í•™ìŠµ

**ê°•í™” í•™ìŠµ(Reinforcement Learning)**
- ì •ì˜: í™˜ê²½ê³¼ì˜ ìƒí˜¸ì‘ìš©ì„ í†µí•´ ìµœì  í–‰ë™ ì •ì±… í•™ìŠµ
- íŠ¹ì§•: ë³´ìƒê³¼ í˜ë„í‹°, ì‹œí–‰ì°©ì˜¤ í•™ìŠµ
- ì˜ˆì‹œ: ê²Œì„ AI, ë¡œë´‡ ì œì–´
- LLM ì ìš©: ì¸ê°„ í”¼ë“œë°±ì„ í†µí•œ ê°•í™” í•™ìŠµ(RLHF)

**ì¤€ì§€ë„ í•™ìŠµ(Semi-supervised Learning)**
- ì •ì˜: ì†ŒëŸ‰ì˜ ë ˆì´ë¸”ëœ ë°ì´í„°ì™€ ëŒ€ëŸ‰ì˜ ë ˆì´ë¸” ì—†ëŠ” ë°ì´í„° í™œìš©
- LLM ì ìš©: ìê¸° ì§€ë„ í•™ìŠµ, ë§ˆìŠ¤í¬ë“œ ì–¸ì–´ ëª¨ë¸ë§

### 2. ì§€ë„ í•™ìŠµ (35ë¶„)

#### íšŒê·€(Regression)
**ê°œë…**
- ì •ì˜: ì—°ì†ì ì¸ ëª©í‘œê°’ ì˜ˆì¸¡
- ëª©í‘œ: ì…ë ¥ ë³€ìˆ˜ì™€ ì¶œë ¥ ë³€ìˆ˜ ê°„ì˜ í•¨ìˆ˜ ê´€ê³„ ëª¨ë¸ë§
- í‰ê°€ ì§€í‘œ: í‰ê·  ì œê³± ì˜¤ì°¨(MSE), í‰ê·  ì ˆëŒ€ ì˜¤ì°¨(MAE), ê²°ì • ê³„ìˆ˜(RÂ²)

**ì„ í˜• íšŒê·€(Linear Regression)**
- ëª¨ë¸: $y = \mathbf{w}^T\mathbf{x} + b$ [ğŸ“–](mathematical_symbols_explanation.md##%201.%20ì„ í˜•%20íšŒê·€%20(Linear%20Regression))
- ì†ì‹¤ í•¨ìˆ˜: $L = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$ [ğŸ“–](mathematical_symbols_explanation.md##%201.%20ì„ í˜•%20íšŒê·€%20(Linear%20Regression))
- ìµœì í™”: ì •ê·œ ë°©ì •ì‹ ë˜ëŠ” ê²½ì‚¬ í•˜ê°•ë²•
- LLM ì ìš©: ì–¸ì–´ ëª¨ë¸ì˜ ì¶œë ¥ ë ˆì´ì–´, íšŒê·€ í—¤ë“œ

**ë¹„ì„ í˜• íšŒê·€**
- ë‹¤í•­ íšŒê·€: $y = \sum_{i=0}^{d} w_i x^i$ [ğŸ“–](mathematical_symbols_explanation.md##%202.%20ë‹¤í•­%20íšŒê·€%20(Polynomial%20Regression))
- ê²°ì • íŠ¸ë¦¬ íšŒê·€: íŠ¸ë¦¬ êµ¬ì¡°ë¥¼ ì´ìš©í•œ ë¹„ì„ í˜• ëª¨ë¸ë§
- ì‹ ê²½ë§ íšŒê·€: ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ ì„ ì´ìš©í•œ ë³µì¡í•œ í•¨ìˆ˜ ê·¼ì‚¬

#### ë¶„ë¥˜(Classification)
**ê°œë…**
- ì •ì˜: ì´ì‚°ì ì¸ í´ë˜ìŠ¤ ë ˆì´ë¸” ì˜ˆì¸¡
- ì´ì§„ ë¶„ë¥˜: ë‘ ê°œì˜ í´ë˜ìŠ¤ (ì˜ˆ: ìŠ¤íŒ¸/ì •ìƒ)
- ë‹¤ì¤‘ ë¶„ë¥˜: ì„¸ ê°œ ì´ìƒì˜ í´ë˜ìŠ¤ (ì˜ˆ: ê°ì„± ë¶„ë¥˜)
- ë‹¤ì¤‘ ë ˆì´ë¸” ë¶„ë¥˜: í•˜ë‚˜ì˜ ìƒ˜í”Œì´ ì—¬ëŸ¬ í´ë˜ìŠ¤ì— ì†í•  ìˆ˜ ìˆìŒ

**ë¡œì§€ìŠ¤í‹± íšŒê·€(Logistic Regression)**
- ëª¨ë¸: $P(y=1|\mathbf{x}) = \sigma(\mathbf{w}^T\mathbf{x} + b)$ [ğŸ“–](mathematical_symbols_explanation.md##%203.%20ë¡œì§€ìŠ¤í‹±%20íšŒê·€%20(Logistic%20Regression))
- ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜: $\sigma(z) = \frac{1}{1 + e^{-z}}$ [ğŸ“–](mathematical_symbols_explanation.md##%203.%20ë¡œì§€ìŠ¤í‹±%20íšŒê·€%20(Logistic%20Regression))
- ì†ì‹¤ í•¨ìˆ˜: êµì°¨ ì—”íŠ¸ë¡œí”¼ $L = -\frac{1}{n}\sum_{i=1}^{n}[y_i\log\hat{y}_i + (1-y_i)\log(1-\hat{y}_i)]$ [ğŸ“–](mathematical_symbols_explanation.md##%203.%20ë¡œì§€ìŠ¤í‹±%20íšŒê·€%20(Logistic%20Regression))
- LLM ì ìš©: ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡, í…ìŠ¤íŠ¸ ë¶„ë¥˜

**ê²°ì • íŠ¸ë¦¬(Decision Trees)**
- ì›ë¦¬: íŠ¹ì„± ê³µê°„ì„ ì¬ê·€ì ìœ¼ë¡œ ë¶„í• 
- ì •ë³´ ì´ë“: $IG = H_{\text{parent}} - \sum_{\text{children}} \frac{N_{\text{child}}}{N_{\text{parent}}} H_{\text{child}}$ [ğŸ“–](mathematical_symbols_explanation.md##%204.%20ê²°ì •%20íŠ¸ë¦¬%20(Decision%20Trees))
- ì§€ë‹ˆ ë¶ˆìˆœë„: $G = 1 - \sum_{i=1}^{C} p_i^2$ [ğŸ“–](mathematical_symbols_explanation.md##%204.%20ê²°ì •%20íŠ¸ë¦¬%20(Decision%20Trees))
- ì¥ì : í•´ì„ ê°€ëŠ¥ì„±, ë¹„ì„ í˜•ì„± ì²˜ë¦¬
- ë‹¨ì : ê³¼ì í•© ê²½í–¥

**ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹ (Support Vector Machines)**
- ì›ë¦¬: ë§ˆì§„ ìµœëŒ€í™”ë¥¼ í†µí•œ ë¶„ë¦¬ ì´ˆí‰ë©´ ì°¾ê¸°
- í•˜ë“œ ë§ˆì§„: ì™„ë²½í•œ ë¶„ë¦¬
- ì†Œí”„íŠ¸ ë§ˆì§„: ì˜¤ë¥˜ í—ˆìš©
- ì»¤ë„ íŠ¸ë¦­: ë¹„ì„ í˜• ë§¤í•‘ì„ í†µí•œ ê³ ì°¨ì› ê³µê°„ì—ì„œì˜ ë¶„ë¦¬
- LLM ì ìš©: í…ìŠ¤íŠ¸ ë¶„ë¥˜, ì˜ë¯¸ ë¶„ì„

### 3. ë¹„ì§€ë„ í•™ìŠµ (20ë¶„)

#### í´ëŸ¬ìŠ¤í„°ë§(Clustering)
**K-í‰ê· (K-Means)**
- ì›ë¦¬: ê±°ë¦¬ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§
- ì•Œê³ ë¦¬ì¦˜:
  1. ì´ˆê¸° ì¤‘ì‹¬ì  ì„ íƒ
  2. ê° ë°ì´í„°ë¥¼ ê°€ì¥ ê°€ê¹Œìš´ ì¤‘ì‹¬ì ì— í• ë‹¹
  3. ì¤‘ì‹¬ì  ì¬ê³„ì‚°
  4. ìˆ˜ë ´í•  ë•Œê¹Œì§€ ë°˜ë³µ
- ëª©ì  í•¨ìˆ˜: $J = \sum_{i=1}^{k}\sum_{\mathbf{x} \in C_i} \|\mathbf{x} - \boldsymbol{\mu}_i\|^2$ [ğŸ“–](mathematical_symbols_explanation.md##%205.%20K-í‰ê· %20í´ëŸ¬ìŠ¤í„°ë§%20(K-Means%20Clustering))
- LLM ì ìš©: ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§, í† í”½ ëª¨ë¸ë§

**ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§(Hierarchical Clustering)**
- ì‘ì§‘ì : ë°”ë‹¥ì—ì„œ ìœ„ë¡œ ë³‘í•©
- ë¶„í• ì : ìœ„ì—ì„œ ì•„ë˜ë¡œ ë¶„í• 
- ë´ë“œë¡œê·¸ë¨: í´ëŸ¬ìŠ¤í„°ë§ ê³¼ì • ì‹œê°í™”
- LLM ì ìš©: ì˜ë¯¸ ìœ ì‚¬ì„±ì— ê¸°ë°˜í•œ ë‹¨ì–´ ê³„ì¸µ êµ¬ì¡°

#### ì°¨ì› ì¶•ì†Œ(Dimensionality Reduction)
**ì£¼ì„±ë¶„ ë¶„ì„(Principal Component Analysis, PCA)**
- ì›ë¦¬: ë°ì´í„°ì˜ ë¶„ì‚°ì„ ìµœëŒ€ë¡œ ë³´ì¡´í•˜ëŠ” ì €ì°¨ì› íˆ¬ì˜
- ê³ ìœ ê°’ ë¶„í•´ë¥¼ í†µí•œ ì£¼ì„±ë¶„ ì°¾ê¸°
- LLM ì ìš©: ë‹¨ì–´ ì„ë² ë”© ì°¨ì› ì¶•ì†Œ, ì‹œê°í™”

**t-SNE (t-Distributed Stochastic Neighbor Embedding)**
- ì›ë¦¬: ê³ ì°¨ì› ë°ì´í„°ì˜ êµ­ì†Œ êµ¬ì¡° ë³´ì¡´
- LLM ì ìš©: ë‹¨ì–´ ì„ë² ë”© ì‹œê°í™”, í´ëŸ¬ìŠ¤í„° êµ¬ì¡° í™•ì¸

### 4. ê³¼ì í•©ê³¼ ì •ê·œí™” (15ë¶„)

#### ê³¼ì í•©(Overfitting)
**ì •ì˜ì™€ ì›ì¸**
- ì •ì˜: í›ˆë ¨ ë°ì´í„°ì— ê³¼ë„í•˜ê²Œ ì ì‘í•˜ì—¬ ì¼ë°˜í™” ì„±ëŠ¥ ì €í•˜
- ì›ì¸: ëª¨ë¸ ë³µì¡ë„ ê³¼ëŒ€, ë°ì´í„° ë¶€ì¡±, ë…¸ì´ì¦ˆ ê³¼ì í•©
- ì¦ìƒ: í›ˆë ¨ ì„±ëŠ¥ì€ ë†’ì§€ë§Œ ê²€ì¦/í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ì€ ë‚®ìŒ

**ê³¼ì í•© ì§„ë‹¨**
- í•™ìŠµ ê³¡ì„ : í›ˆë ¨ ì†ì‹¤ê³¼ ê²€ì¦ ì†ì‹¤ì˜ ê°„ê²©
- êµì°¨ ê²€ì¦: ë°ì´í„° ë¶„í• ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™”
- LLM ì ìš©: í›ˆë ¨ ì†ì‹¤ê³¼ ê²€ì¦ ì†ì‹¤ ëª¨ë‹ˆí„°ë§

#### ì •ê·œí™”(Regularization)
**L1 ì •ê·œí™” (Lasso)**
- ì†ì‹¤ í•¨ìˆ˜: $L = \text{Original Loss} + \lambda\sum_{i}|w_i|$ [ğŸ“–](mathematical_symbols_explanation.md##%206.%20ì •ê·œí™”%20(Regularization))
- íŠ¹ì§•: í¬ì†Œì„± ìœ ë„, íŠ¹ì„± ì„ íƒ
- LLM ì ìš©: ëª¨ë¸ ì••ì¶•, ì¤‘ìš”í•œ íŒŒë¼ë¯¸í„° ì‹ë³„

**L2 ì •ê·œí™” (Ridge)**
- ì†ì‹¤ í•¨ìˆ˜: $L = \text{Original Loss} + \lambda\sum_{i}w_i^2$ [ğŸ“–](mathematical_symbols_explanation.md##%206.%20ì •ê·œí™”%20(Regularization))
- íŠ¹ì§•: ê°€ì¤‘ì¹˜ ê°ì‡ , ì‘ì€ ê°€ì¤‘ì¹˜ ì„ í˜¸
- LLM ì ìš©: ê°€ì¤‘ì¹˜ ê°ì‡ , ê³¼ì í•© ë°©ì§€

**ë“œë¡­ì•„ì›ƒ(Dropout)**
- ì›ë¦¬: í›ˆë ¨ ì¤‘ ë¬´ì‘ìœ„ë¡œ ë‰´ëŸ° ë¹„í™œì„±í™”
- íš¨ê³¼: ì•™ìƒë¸” íš¨ê³¼, ê³¼ì í•© ë°©ì§€
- LLM ì ìš©: íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ë“œë¡­ì•„ì›ƒ ë ˆì´ì–´

**ì¡°ê¸° ì¢…ë£Œ(Early Stopping)**
- ì›ë¦¬: ê²€ì¦ ì„±ëŠ¥ì´ ë” ì´ìƒ í–¥ìƒë˜ì§€ ì•Šì„ ë•Œ í›ˆë ¨ ì¤‘ë‹¨
- LLM ì ìš©: ìµœì  í›ˆë ¨ ì‹œì  ê²°ì •

## ì‹¤ìŠµ ì„¸ì…˜ (90ë¶„)

### 1. scikit-learnì„ ì´ìš©í•œ ê¸°ë³¸ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ (30ë¶„)

#### ì„ í˜• íšŒê·€ êµ¬í˜„
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

# ë°ì´í„° ìƒì„±
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ì„ í˜• íšŒê·€ ëª¨ë¸ í•™ìŠµ
model = LinearRegression()
model.fit(X_train, y_train)

# ì˜ˆì¸¡
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

# í‰ê°€
train_mse = mean_squared_error(y_train, y_train_pred)
test_mse = mean_squared_error(y_test, y_test_pred)
train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"í›ˆë ¨ MSE: {train_mse:.4f}, RÂ²: {train_r2:.4f}")
print(f"í…ŒìŠ¤íŠ¸ MSE: {test_mse:.4f}, RÂ²: {test_r2:.4f}")
print(f"ê¸°ìš¸ê¸°: {model.coef_[0][0]:.4f}, ì ˆí¸: {model.intercept_[0]:.4f}")

# ì‹œê°í™”
plt.figure(figsize=(10, 6))
plt.scatter(X_train, y_train, alpha=0.7, label='Training data')
plt.scatter(X_test, y_test, alpha=0.7, label='Test data')
plt.plot(X, model.predict(X), 'r-', label='Linear regression')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Linear Regression')
plt.legend()
plt.grid(True)
plt.show()
```

#### ë¡œì§€ìŠ¤í‹± íšŒê·€ êµ¬í˜„
```python
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# ë°ì´í„° ìƒì„±
X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, 
                          n_informative=2, n_clusters_per_class=1, random_state=42)

# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ í•™ìŠµ
model = LogisticRegression()
model.fit(X_train, y_train)

# ì˜ˆì¸¡
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

# í‰ê°€
train_acc = accuracy_score(y_train, y_train_pred)
test_acc = accuracy_score(y_test, y_test_pred)

print(f"í›ˆë ¨ ì •í™•ë„: {train_acc:.4f}")
print(f"í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.4f}")
print("\në¶„ë¥˜ ë³´ê³ ì„œ:")
print(classification_report(y_test, y_test_pred))

# ê²°ì • ê²½ê³„ ì‹œê°í™”
def plot_decision_boundary(model, X, y):
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                         np.arange(y_min, y_max, 0.02))
    
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    plt.figure(figsize=(10, 6))
    plt.contourf(xx, yy, Z, alpha=0.3)
    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title('Logistic Regression Decision Boundary')
    plt.show()

plot_decision_boundary(model, X, y)
```

### 2. ê³¼ì í•©ê³¼ ì •ê·œí™” ì‹¤í—˜ (30ë¶„)

#### ë‹¤í•­ íšŒê·€ì—ì„œì˜ ê³¼ì í•©
```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline

# ë¹„ì„ í˜• ë°ì´í„° ìƒì„±
np.random.seed(42)
X = 6 * np.random.rand(100, 1) - 3
y = 0.5 * X**2 + X + 2 + np.random.randn(100, 1)

# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ë‹¤ì–‘í•œ ì°¨ìˆ˜ì˜ ë‹¤í•­ íšŒê·€ ëª¨ë¸
degrees = [1, 2, 10, 15]
plt.figure(figsize=(15, 10))

for i, degree in enumerate(degrees):
    # íŒŒì´í”„ë¼ì¸ ìƒì„±
    polynomial_features = PolynomialFeatures(degree=degree, include_bias=False)
    linear_regression = LinearRegression()
    pipeline = Pipeline([
        ("polynomial_features", polynomial_features),
        ("linear_regression", linear_regression)
    ])
    
    # ëª¨ë¸ í•™ìŠµ
    pipeline.fit(X_train, y_train)
    
    # ì˜ˆì¸¡
    y_train_pred = pipeline.predict(X_train)
    y_test_pred = pipeline.predict(X_test)
    
    # í‰ê°€
    train_mse = mean_squared_error(y_train, y_train_pred)
    test_mse = mean_squared_error(y_test, y_test_pred)
    
    # ì‹œê°í™”
    plt.subplot(2, 2, i + 1)
    X_smooth = np.linspace(-3, 3, 100).reshape(-1, 1)
    y_smooth = pipeline.predict(X_smooth)
    
    plt.scatter(X_train, y_train, alpha=0.7, label='Training data')
    plt.scatter(X_test, y_test, alpha=0.7, label='Test data')
    plt.plot(X_smooth, y_smooth, 'r-', label=f'Degree {degree}')
    plt.xlabel('X')
    plt.ylabel('y')
    plt.title(f'Degree {degree}: Train MSE={train_mse:.2f}, Test MSE={test_mse:.2f}')
    plt.legend()
    plt.grid(True)

plt.tight_layout()
plt.show()
```

#### ì •ê·œí™” íš¨ê³¼ ë¹„êµ
```python
from sklearn.linear_model import Ridge, Lasso
from sklearn.preprocessing import StandardScaler

# ë°ì´í„° ìƒì„±
np.random.seed(42)
n_samples, n_features = 100, 10
X = np.random.randn(n_samples, n_features)
# ì‹¤ì œ ì¤‘ìš”í•œ íŠ¹ì„±ì€ 3ê°œë§Œ
true_coef = np.array([3, -2, 1.5] + [0] * (n_features - 3))
y = X @ true_coef + np.random.randn(n_samples) * 0.5

# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# íŠ¹ì„± ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ë‹¤ì–‘í•œ ì •ê·œí™” ê°•ë„ë¡œ ëª¨ë¸ í•™ìŠµ
alphas = [0, 0.01, 0.1, 1, 10]

plt.figure(figsize=(15, 5))

# ì„ í˜• íšŒê·€ (ì •ê·œí™” ì—†ìŒ)
plt.subplot(1, 3, 1)
lr = LinearRegression()
lr.fit(X_train_scaled, y_train)
plt.bar(range(n_features), lr.coef_)
plt.title('Linear Regression (No Regularization)')
plt.xlabel('Feature Index')
plt.ylabel('Coefficient Value')
plt.xticks(range(n_features))

# Ridge ì •ê·œí™”
plt.subplot(1, 3, 2)
ridge_coefs = []
for alpha in alphas:
    ridge = Ridge(alpha=alpha)
    ridge.fit(X_train_scaled, y_train)
    ridge_coefs.append(ridge.coef_)

ridge_coefs = np.array(ridge_coefs)
for i in range(n_features):
    plt.plot(alphas, ridge_coefs[:, i], label=f'Feature {i}')
plt.xscale('log')
plt.title('Ridge Regularization')
plt.xlabel('Alpha')
plt.ylabel('Coefficient Value')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')

# Lasso ì •ê·œí™”
plt.subplot(1, 3, 3)
lasso_coefs = []
for alpha in alphas:
    lasso = Lasso(alpha=alpha)
    lasso.fit(X_train_scaled, y_train)
    lasso_coefs.append(lasso.coef_)

lasso_coefs = np.array(lasso_coefs)
for i in range(n_features):
    plt.plot(alphas, lasso_coefs[:, i], label=f'Feature {i}')
plt.xscale('log')
plt.title('Lasso Regularization')
plt.xlabel('Alpha')
plt.ylabel('Coefficient Value')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')

plt.tight_layout()
plt.show()
```

### 3. êµì°¨ ê²€ì¦ê³¼ ëª¨ë¸ ì„ íƒ (30ë¶„)

#### K-í´ë“œ êµì°¨ ê²€ì¦
```python
from sklearn.model_selection import cross_val_score, KFold
from sklearn.datasets import load_iris
from sklearn.neighbors import KNeighborsClassifier

# ë°ì´í„° ë¡œë“œ
iris = load_iris()
X, y = iris.data, iris.target

# K-í´ë“œ êµì°¨ ê²€ì¦
k_values = range(1, 31)
cv_scores = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X, y, cv=5, scoring='accuracy')
    cv_scores.append(scores.mean())

# ìµœì ì˜ k ê°’ ì°¾ê¸°
optimal_k = k_values[np.argmax(cv_scores)]
max_score = max(cv_scores)

print(f"ìµœì ì˜ k: {optimal_k}, ìµœê³  êµì°¨ ê²€ì¦ ì •í™•ë„: {max_score:.4f}")

# ì‹œê°í™”
plt.figure(figsize=(10, 6))
plt.plot(k_values, cv_scores)
plt.xlabel('Number of Neighbors (k)')
plt.ylabel('Cross-Validation Accuracy')
plt.title('KNN: Optimal k Selection')
plt.grid(True)
plt.show()
```

#### í•™ìŠµ ê³¡ì„ ì„ í†µí•œ ê³¼ì í•© ì§„ë‹¨
```python
from sklearn.model_selection import learning_curve
from sklearn.svm import SVC

# í•™ìŠµ ê³¡ì„  ìƒì„±
def plot_learning_curve(estimator, title, X, y, cv=None, train_sizes=np.linspace(0.1, 1.0, 5)):
    plt.figure(figsize=(10, 6))
    
    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, cv=cv, train_sizes=train_sizes, scoring='accuracy')
    
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)
    
    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                     train_scores_mean + train_scores_std, alpha=0.1, color="r")
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                     test_scores_mean + test_scores_std, alpha=0.1, color="g")
    
    plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training score")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g", label="Cross-validation score")
    
    plt.xlabel("Training examples")
    plt.ylabel("Accuracy Score")
    plt.title(title)
    plt.legend(loc="best")
    plt.grid(True)
    plt.show()

# SVM ëª¨ë¸ì˜ í•™ìŠµ ê³¡ì„ 
plot_learning_curve(SVC(gamma=0.01), "Learning Curve (SVM, gamma=0.01)", X, y, cv=5)
plot_learning_curve(SVC(gamma=0.001), "Learning Curve (SVM, gamma=0.001)", X, y, cv=5)
```

## ê³¼ì œ

### 1. íšŒê·€ ê³¼ì œ
- ë³´ìŠ¤í„´ ì£¼íƒ ê°€ê²© ë°ì´í„°ì…‹ì— ëŒ€í•œ ë‹¤ì–‘í•œ íšŒê·€ ëª¨ë¸ ë¹„êµ
- ë‹¤í•­ íšŒê·€ì—ì„œì˜ ìµœì  ì°¨ìˆ˜ ì„ íƒ
- ì •ê·œí™” ê°•ë„ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™” ë¶„ì„

### 2. ë¶„ë¥˜ ê³¼ì œ
- ì™€ì¸ ë°ì´í„°ì…‹ì— ëŒ€í•œ ë‹¤ì–‘í•œ ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ ë¹„êµ
- íŠ¹ì„± ìŠ¤ì¼€ì¼ë§ì´ ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë¶„ì„
- í˜¼ë™ í–‰ë ¬ì„ í†µí•œ ë¶„ë¥˜ ê²°ê³¼ ìƒì„¸ ë¶„ì„

### 3. ë¹„ì§€ë„ í•™ìŠµ ê³¼ì œ
- K-í‰ê·  í´ëŸ¬ìŠ¤í„°ë§ì—ì„œ ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ì°¾ê¸°
- PCAë¥¼ ì´ìš©í•œ ì°¨ì› ì¶•ì†Œ ë° ì‹œê°í™”
- t-SNEì™€ PCAì˜ ì°¨ì´ì  ë¹„êµ ë¶„ì„

## ì¶”ê°€ í•™ìŠµ ìë£Œ

### ì˜¨ë¼ì¸ ê°•ì˜
- [Andrew Ng's Machine Learning Course](https://www.coursera.org/learn/machine-learning)
- [StatQuest with Josh Starmer](https://www.youtube.com/c/statquest)
- [Machine Learning Mastery](https://machinelearningmastery.com/)

### êµì¬
- "Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow" by AurÃ©lien GÃ©ron
- "Pattern Recognition and Machine Learning" by Christopher M. Bishop
- "The Elements of Statistical Learning" by Trevor Hastie, Robert Tibshirani, Jerome Friedman

### ì˜¨ë¼ì¸ ìë£Œ
- [Scikit-learn Documentation](https://scikit-learn.org/stable/documentation.html)
- [Papers with Code](https://paperswithcode.com/)
- [Kaggle Learn](https://www.kaggle.com/learn)