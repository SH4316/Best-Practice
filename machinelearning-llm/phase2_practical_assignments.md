# Phase 2: 딥러닝 심화와 트랜스포머 - 실습 과제 모음

## 과제 개요

Phase 2의 실습 과제는 심화 딥러닝 아키텍처와 트랜스포머의 원리를 깊이 이해하고 직접 구현해보는 것을 목표로 합니다. 각 주차별로 제공된 이론 내용을 실제 코드로 구현하며, LLM의 핵심 기술인 트랜스포머 아키텍처에 대한 체계적인 이해를 도모합니다.

## 평가 기준

- **구현 완성도 (40%)**: 요구된 기능이 올바르게 구현되었는가
- **이론적 이해 (30%)**: 구현 뒤에 있는 수학적 원리를 이해하고 설명할 수 있는가
- **실험과 분석 (20%)**: 체계적인 실험 설계와 의미 있는 분석을 수행했는가
- **코드 품질 (10%)**: 코드 가독성, 효율성, 재사용성이 우수한가

---

## 5주차 과제: 심화 딥러닝 아키텍처

### 과제 1: CNN 심화 구현 (25점)

**목표**: 다양한 CNN 아키텍처의 원리 이해와 구현

**기본 요구사항**:
1. **ResNet 기본 블록 구현**
   - 잔차 연결(Residual Connection) 구현
   - 병목(Bottleneck) 아키텍처 구현
   - 배치 정규화와 ReLU 활성화 함수 적용

2. **CIFAR-10 데이터셋에 대한 ResNet 훈련**
   - 데이터 증강(Data Augmentation) 적용
   - 학습률 스케줄링 구현
   - 과적합 방지 기법 적용

3. **특성 맵 시각화와 분석**
   - 다양한 레이어의 특성 맵 시각화
   - 필터 가중치 분석
   - 클래스 활성화 맵(Class Activation Map) 생성

**제출물**:
- `resnet_implementation.ipynb`: ResNet 구현과 훈련 과정
- `feature_visualization/`: 특성 맵 시각화 결과
- `performance_analysis/`: 성능 분석과 비교 결과

**추가 도전 과제 (최대 5점 가산점)**:
- **EfficientNet 기본 아키텍처 구현**: 복합 스케일링(Compound Scaling) 적용
- **전이 학습 실험**: ImageNet 사전 훈련 모델 미세조정
- **모델 압축**: 지식 증류(Knowledge Distillation) 또는 가지치기(Pruning)

### 과제 2: RNN/LSTM 심화 구현 (25점)

**목표**: 순환 신경망의 시퀀스 처리 메커니즘 심화 이해

**기본 요구사항**:
1. **LSTM 셀 처음부터 구현**
   - 게이트 메커니즘(망각, 입력, 출력 게이트) 구현
   - 셀 상태와 은닉 상태 분리
   - 그래디언트 클리핑 적용

2. **시계열 데이터 예측 모델**
   - 주식 가격, 날씨 데이터 등 실제 시계열 데이터 적용
   - 다중 스텝 예측(Multi-step Forecasting) 구현
   - 양방향 LSTM(Bidirectional LSTM) 적용

3. **게이트 활성화 분석**
   - 각 게이트의 활성화 패턴 시각화
   - 장단기 기억 메커니즘 분석
   - 그래디언트 흐름 시각화

**제출물**:
- `lstm_implementation.ipynb`: LSTM 구현과 시계열 예측
- `gate_analysis/`: 게이트 활성화 분석 결과
- `time_series_forecasting/`: 시계열 예측 결과

**추가 도전 과제 (최대 5점 가산점)**:
- **GRU와 LSTM 비교**: 두 아키텍처의 성능과 계산 효율성 비교
- **어텐션 기반 RNN**: 어텐션 메커니즘을 RNN에 통합
- **변분 오토인코더**: LSTM을 이용한 시퀀스 생성 모델

### 과제 3: 어텐션 메커니즘 구현 (20점)

**목표**: 어텐션 메커니즘의 원리 이해와 다양한 변형 구현

**기본 요구사항**:
1. **기본 어텐션 메커니즘 구현**
   - 콘텐츠 기반 어텐션(Content-based Attention)
   - 위치 기반 어텐션(Location-based Attention)
   - 하이브리드 어텐션(Hybrid Attention)

2. **시퀀스-투-시퀀스 모델에 어텐션 적용**
   - 인코더-디코더 어텐션 구현
   - 글로벌 vs 로컬 어텐션 비교
   - 커버리지 메커니즘(Coverage Mechanism) 구현

3. **어텐션 가중치 분석**
   - 다양한 입력에 대한 어텐션 패턴 분석
   - 어텐션의 해석 가능성 연구
   - 잘못된 어텐션 사례 분석

**제출물**:
- `attention_mechanisms.ipynb`: 어텐션 메커니즘 구현
- `attention_visualization/`: 어텐션 가중치 시각화
- `seq2seq_model/`: 시퀀스-투-시퀀스 모델

**추가 도전 과제 (최대 5점 가산점)**:
- **멀티-헤드 어텐션 구현**: 여러 어텐션 헤드의 병렬 처리
- **자기 어텐션(Self-Attention)**: 트랜스포머의 핵심 메커니즘
- **효율적 어텐션**: 메모리 효율적인 어텐션 변형 구현

---

## 6주차 과제: 트랜스포머 아키텍처 (1)

### 과제 1: 셀프 어텐션 심화 구현 (30점)

**목표**: 셀프 어텐션 메커니즘의 깊은 이해와 최적화

**기본 요구사항**:
1. **스케일드 닷 프로덕트 어텐션 최적화**
   - 수치적 안정성 개선 (FP16 혼합 정밀도)
   - 메모리 효율적 구현 (Chunked Attention)
   - 병렬 처리 최적화

2. **다양한 마스킹 기법 구현**
   - 패딩 마스크(Padding Mask)
   - 미래 마스크(Causal Mask)
   - 사용자 정의 마스크 지원

3. **어텐션 계산 효율성 분석**
   - 시간 복잡도 측정과 최적화
   - 메모리 사용량 분석
   - 배치 크기에 따른 성능 비교

**제출물**:
- `self_attention_optimized.ipynb`: 최적화된 셀프 어텐션
- `performance_benchmark/`: 성능 벤치마크 결과
- `masking_techniques/`: 다양한 마스킹 기법 구현

**추가 도전 과제 (최대 5점 가산점)**:
- **Flash Attention 구현**: 메모리 효율적인 어텐션 알고리즘
- **희소 어텐션(Sparse Attention)**: 선택적 어텐션 계산
- **선형 어텐션(Linear Attention)**: 선형 시간 복잡도 어텐션

### 과제 2: 멀티-헤드 어텐션 심화 분석 (30점)

**목표**: 멀티-헤드 어텐션의 동작 원리와 특성 분석

**기본 요구사항**:
1. **헤드별 특성화 분석**
   - 각 헤드가 학습하는 패턴 분석
   - 헤드 수에 따른 성능 변화 측정
   - 헤드 간 상관관계 분석

2. **헤드 가중치 학습**
   - 학습 가능한 헤드 가중치 구현
   - 헤드 중요도 동적 조절
   - 헤드 선택 메커니즘

3. **어텐션 패턴 시각화**
   - 헤드별 어텐션 맵 시각화
   - 레이어별 어텐션 변화 추적
   - 입력에 따른 어텐션 패턴 변화 분석

**제출물**:
- `multihead_analysis.ipynb`: 멀티-헤드 어텐션 분석
- `head_patterns/`: 헤드별 패턴 시각화
- `head_importance/`: 헤드 중요도 분석 결과

**추가 도전 과제 (최대 5점 가산점)**:
- **헤드 초기화 전략**: 다양한 초기화 방법의 효과 비교
- **동적 헤드 수**: 입력에 따라 헤드 수 동적 조절
- **헤드 전문화**: 특정 작업에 헤드 전문화 유도

---

## 7주차 과제: 트랜스포머 아키텍처 (2)

### 과제 1: 위치 인코딩 비교 분석 (25점)

**목표**: 다양한 위치 인코딩 방법의 특성과 효과 비교

**기본 요구사항**:
1. **다양한 위치 인코딩 구현**
   - 사인/코사인 위치 인코딩
   - 학습 가능한 위치 인코딩
   - 상대적 위치 인코딩(Relative Positional Encoding)
   - 회전 위치 인코딩(Rotary Position Embedding)

2. **위치 인코딩 특성 분석**
   - 상대적 위치 관계 학습 능력 비교
   - 긴 시퀀스에 대한 일반화 성능
   - 계산 효율성과 메모리 사용량

3. **다양한 길이의 시퀀스 실험**
   - 훈련 길이보다 긴 시퀀스 처리 능력
   - 위치 정보 보존 정도 분석
   - 외삽(Extrapolation) 성능 측정

**제출물**:
- `positional_encoding_comparison.ipynb`: 위치 인코딩 비교
- `encoding_analysis/`: 위치 인코딩 특성 분석
- `length_generalization/`: 길이 일반화 실험 결과

**추가 도전 과제 (최대 5점 가산점)**:
- **적응적 위치 인코딩**: 입력에 따라 동적 조절되는 위치 인코딩
- **다국어 위치 인코딩**: 다양한 언어에 최적화된 위치 인코딩
- **위치 인코딩 압축**: 효율적인 위치 정보 표현

### 과제 2: 인코더-디코더 구조 구현 (25점)

**목표**: 완전한 트랜스포머 인코더-디코더 구조 구현

**기본 요구사항**:
1. **트랜스포머 인코더 구현**
   - 멀티-헤드 셀프 어텐션 레이어
   - 포지션 와이즈 피드포워드 네트워크
   - 잔차 연결과 레이어 정규화

2. **트랜스포머 디코더 구현**
   - 마스크드 멀티-헤드 셀프 어텐션
   - 인코더-디코더 어텐션
   - 자기 회귀(autoregressive) 생성

3. **기계 번역 모델 훈련**
   - 영어-독일어 번역 데이터셋 적용
   - 교사 강요(Teacher Forcing) 구현
   - 빔 서치(Beam Search) 디코딩

**제출물**:
- `transformer_enc_dec.ipynb`: 인코더-디코더 구현
- `translation_model/`: 기계 번역 모델
- `decoding_strategies/`: 다양한 디코딩 전략 비교

**추가 도전 과제 (최대 5점 가산점)**:
- **인코더 전용 모델**: BERT 스타일 인코더 구현
- **디코더 전용 모델**: GPT 스타일 디코더 구현
- **효율적 디코딩**: 캐싱과 병렬 디코딩 최적화

---

## 8주차 과제: 중간 프로젝트

### 중간 프로젝트: 간단한 트랜스포머 처음부터 구현 (100점)

**목표**: Phase 1과 Phase 2에서 학습한 모든 개념을 통합하여 완전한 트랜스포머 모델 구현

**프로젝트 요구사항**: (상세 내용은 `phase2_week8_midterm_project.md` 참조)

1. **기본 구현 (60점)**
   - 셀프 어텐션과 멀티-헤드 어텐션
   - 위치 인코딩과 트랜스포머 블록
   - 완전한 인코더-디코더 구조

2. **응용 구현 (25점)**
   - 실제 NLP 과제 선택과 데이터 전처리
   - 모델 훈련과 성능 평가
   - 기준 모델과의 성능 비교

3. **심화 분석 (15점)**
   - 어텐션 가중치 시각화와 분석
   - 모델 개선 실험
   - 하이퍼파라미터 튜닝

**제출물**:
- **코드**: 완전한 트랜스포머 구현 코드
- **보고서**: 기술 보고서와 실험 노트북
- **발표**: 15분 발표 자료와 시연

---

## Phase 2 종합 평가

### 평가 방식
- **주별 과제 (70%)**: 각 주차별 과제 수행 평가
- **중간 프로젝트 (30%)**: 종합적인 구현과 분석 능력 평가

### 피드백 방식
- **상세 코드 리뷰**: 각 과제에 대한 구체적인 피드백 제공
- **동료 평가**: 동료 간 코드 리뷰와 피드백 교환
- **개선 제안**: 코드 개선과 성능 향상을 위한 구체적 제안

### 성공적인 과제 수행을 위한 조언

1. **점진적 구현**: 작은 단위로 나누어 구현하고 테스트
2. **이론과 실습의 연결**: 구현 뒤에 있는 수학적 원리 이해
3. **실험 문서화**: 모든 실험 과정과 결과 상세히 기록
4. **시각화 활용**: 복잡한 개념의 시각적 표현
5. **조기 시작**: 마감일에 몰리지 않고 충분한 시간 투자

## 추가 학습 자료

### 구현 참고 자료
- [Harvard's Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
- [PyTorch Transformer Tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)

### 데이터셋
- [Multi30k](https://github.com/multi30k/dataset): 기계 번역
- [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html): 이미지 분류
- [UCI ML Repository](https://archive.ics.uci.edu/ml/datasets.php): 시계열 데이터

### 성능 최적화
- [NVIDIA Apex](https://github.com/NVIDIA/apex): 혼합 정밀도 훈련
- [PyTorch Profiler](https://pytorch.org/tutorials/recipes/recipes/profiler.html): 성능 분석
- [TensorBoard](https://www.tensorflow.org/tensorboard): 훈련 시각화

이 실습 과제들을 통해 학생들은 딥러닝과 트랜스포머 아키텍처에 대한 깊은 이해를 얻고, LLM 개발에 필요한 핵심 기술을 마스터하게 될 것입니다.