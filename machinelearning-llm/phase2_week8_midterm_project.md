# 8주차: 중간 프로젝트 - 간단한 트랜스포머 구현

## 프로젝트 개요

이번 중간 프로젝트는 Phase 1과 Phase 2에서 학습한 모든 개념을 통합하여 간단한 트랜스포머 모델을 처음부터 구현하고, 이를 실제 문제에 적용해보는 것을 목표로 합니다. 학생들은 트랜스포머의 핵심 구성 요소를 직접 구현하며 모델의 동작 원리를 깊이 이해하게 될 것입니다.

## 프로젝트 목표

1. **트랜스포머 핵심 구성 요소 구현**: 셀프 어텐션, 멀티-헤드 어텐션, 위치 인코딩 등
2. **완전한 트랜스포머 모델 구축**: 인코더-디코더 구조의 완전한 구현
3. **실제 문제에 적용**: 기계 번역이나 텍스트 분류와 같은 실제 NLP 과제 해결
4. **모델 분석과 시각화**: 어텐션 가중치, 학습 과정 등의 분석
5. **성능 최적화**: 하이퍼파라미터 튜닝과 모델 개선

## 프로젝트 요구사항

### 1. 기본 구현 요구사항 (60점)

#### 1.1 핵심 구성 요소 구현 (20점)
- **셀프 어텐션 메커니즘**
  - 스케일드 닷 프로덕트 어텐션 구현
  - 마스킹 기능 포함 (패딩 마스크, 미래 마스크)
  - 수치적 안정성 고려

- **멀티-헤드 어텐션**
  - 병렬 어텐션 계산
  - 헤드 결합과 출력 투영
  - 효율적인 행렬 연산

- **위치 인코딩**
  - 사인/코사인 위치 인코딩 구현
  - 학습 가능한 위치 인코딩 옵션
  - 최대 길이 초과 처리

#### 1.2 트랜스포머 블록 구현 (20점)
- **인코더 블록**
  - 멀티-헤드 셀프 어텐션
  - 포지션 와이즈 피드포워드 네트워크
  - 잔차 연결과 레이어 정규화

- **디코더 블록**
  - 마스크드 멀티-헤드 셀프 어텐션
  - 인코더-디코더 어텐션
  - 포지션 와이즈 피드포워드 네트워크

#### 1.3 완전한 모델 구현 (20점)
- **트랜스포머 아키텍처**
  - 임베딩 레이어와 위치 인코딩
  - 인코더와 디코더 스택
  - 최종 출력 레이어

- **훈련 루프**
  - 손실 함수 구현 (교차 엔트로피)
  - 옵티마이저와 학습률 스케줄러
  - 배치 처리와 데이터 로딩

### 2. 응용 구현 요구사항 (25점)

#### 2.1 데이터셋 선택과 전처리 (10점)
- **데이터셋**
  - 기계 번역: 영어-한국어 또는 영어-독일어 쌍
  - 텍스트 분류: 감성 분석, 주제 분류 등
  - 또는 제안하는 다른 NLP 과제

- **전처리**
  - 토크나이징 (단어 또는 서브워드)
  - 어휘 구축과 인덱싱
  - 패딩과 시퀀스 길이 통일

#### 2.2 모델 훈련과 평가 (15점)
- **훈련 과정**
  - 검증 세트를 이용한 성능 모니터링
  - 조기 종료와 모델 저장
  - 학습 곡선 시각화

- **성능 평가**
  - 과제별 적절한 평가 지표 (BLEU, 정확도 등)
  - 기준 모델과의 성능 비교
  - 오류 분석과 개선 제안

### 3. 심화 분석 요구사항 (15점)

#### 3.1 어텐션 분석 (10점)
- **어텐션 가중치 시각화**
  - 헤드별 어텐션 패턴 분석
  - 레이어별 어텐션 변화 관찰
  - 입력 예시에 대한 어텐션 해석

- **위치 인코딩 분석**
  - 다양한 위치 인코딩 방법 비교
  - 위치 정보의 학습 과정 분석
  - 긴 시퀀스에서의 성능 분석

#### 3.2 모델 개선 실험 (5점)
- **하이퍼파라미터 튜닝**
  - 모델 크기 (임베딩 차원, 레이어 수)
  - 학습률과 옵티마이저
  - 정규화 기법 (드롭아웃, 가중치 감쇠)

- **아키텍처 개선**
  - 다른 활성화 함수 실험
  - 레이어 정규화 위치 변경
  - 기타 창의적인 개선 아이디어

## 프로젝트 단계별 가이드

### 1단계: 기본 구조 설계 (1주차)
- 프로젝트 계획서 작성
- 데이터셋 탐색과 선택
- 모델 아키텍처 설계
- 개발 환경 설정

### 2단계: 핵심 구성 요소 구현 (2주차)
- 셀프 어텐션과 멀티-헤드 어텐션 구현
- 위치 인코딩 구현
- 단위 테스트 수행

### 3단계: 트랜스포머 블록 구현 (3주차)
- 인코더와 디코더 블록 구현
- 완전한 모델 조립
- 기본 훈련 루프 구현

### 4단계: 데이터 적용과 훈련 (4주차)
- 데이터 전처리와 로더 구현
- 모델 훈련과 하이퍼파라미터 튜닝
- 성능 평가와 분석

### 5단계: 심화 분석과 보고서 작성 (5주차)
- 어텐션 가중치 분석
- 모델 개선 실험
- 최종 보고서와 발표 자료 준비

## 제출물

### 1. 코드 (40점)
- **구현 코드**
  - 모든 소스 코드와 주석
  - 재현 가능한 실행 환경 (requirements.txt)
  - 단위 테스트 코드

- **실행 결과**
  - 훈련된 모델 가중치
  - 훈련 로그와 성능 지표
  - 시각화 결과물

### 2. 보고서 (40점)
- **기술 보고서 (최소 10페이지)**
  - 프로젝트 개요와 목표
  - 모델 아키텍처 상세 설명
  - 구현 과정과 기술적 난관
  - 실험 결과와 분석
  - 결론과 향후 개선 방향

- **실험 노트북**
  - 데이터 탐색과 전처리 과정
  - 모델 훈련과 평가 코드
  - 시각화와 분석 결과

### 3. 발표 (20점)
- **발표 자료 (15분)**
  - 프로젝트 소개
  - 주요 구현 결과
  - 시연과 성능 비교
  - 결론과 시사점

- **질의응답**
  - 기술적 질문에 대한 답변
  - 구현 선택에 대한 정당화
  - 한계와 개선 방안 논의

## 평가 기준

### 기술적 구현 (50점)
- **코드 완성도 (20점)**: 요구된 기능이 올바르게 구현되었는가
- **모델 정확성 (15점)**: 트랜스포머 원리를 올바르게 구현했는가
- **코드 품질 (15점)**: 코드 가독성, 효율성, 재사용성

### 실험과 분석 (30점)
- **실험 설계 (10점)**: 체계적인 실험 계획과 수행
- **결과 분석 (15점)**: 깊이 있는 분석과 의미 있는 인사이트
- **시각화 품질 (5점)**: 명확하고 정보력 있는 시각화

### 보고서와 발표 (20점)
- **보고서 완성도 (10점)**: 논리적 구조와 상세한 설명
- **발표 명확성 (10점)**: 효과적인 발표와 시연

## 추가 도전 과제 (최대 10점 가산점)

### 1. 고급 기법 구현 (5점)
- **효율적 어텐션**: Sparse Attention, Linear Attention 등
- **개선된 위치 인코딩**: Relative Positional Encoding, Rotary Position Embedding
- **정규화 기법**: LayerNorm 대신 RMSNorm, Pre-Norm vs Post-Norm

### 2. 대규모 실험 (5점)
- **다양한 데이터셋**: 여러 NLP 과제에 대한 실험
- **사전 훈련 모델**: BERT나 GPT와의 성능 비교
- **추론 최적화**: 모델 압축이나 가속화 기법 적용

## 유의사항

### 학업 정책
- **개별 작업 원칙**: 팀 프로젝트이나 코드 공유 금지
- **참고 자료 명시**: 외부 코드나 아이디어 사용 시 반드시 출처 표기
- **학문적 정직성**: 모든 제출물은 본인의 작업이어야 함

### 기술적 요구사항
- **재현성**: 모든 결과는 재현 가능해야 함
- **문서화**: 코드와 실험 과정의 상세한 문서화
- **버전 관리**: Git을 이용한 코드 버전 관리

### 일정 관리
- **중간 발표**: 3주차에 진행 상황 발표
- **최종 제출**: 5주차 금요일 23:59까지
- **지연 제출**: 하루당 10% 감점 (최대 3일까지)

## 도움을 위한 자료

### 기술 참고 자료
- [Harvard's Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
- [PyTorch Transformer Tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)

### 데이터셋
- [Multi30k](https://github.com/multi30k/dataset): 영어-독일어 번역
- [IMDb](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews): 감성 분석
- [AG News](https://github.com/mhjabreel/CharCnn_Keras/tree/master/data/ag_news_csv): 주제 분류

### 구현 팁
- **단계적 구현**: 작은 단위로 나누어 구현하고 테스트
- **디버깅 도구**: print 문, debugger, profiler 적극 활용
- **성능 모니터링**: GPU 메모리 사용량, 훈련 속도 확인

## 성공적인 프로젝트를 위한 조언

1. **시간 관리**: 초기에 충분한 시간 투자, 마감일에 몰리지 않기
2. **점진적 개발**: 작은 단위로 구현하고 테스트하며 진행
3. **지속적인 문서화**: 구현 과정과 결정 사항을 꾸준히 기록
4. **적극적인 질문**: 어려운 부분은 교수나 동료에게 질문
5. **다양한 실험**: 기본 요구사항 외 추가 실험을 통해 깊이 있는 학습

이 중간 프로젝트를 통해 학생들은 트랜스포머의 깊은 이해를 얻고, LLM 개발에 필요한 핵심 기술을 습득하게 될 것입니다. 성공적인 프로젝트 수행을 바랍니다!