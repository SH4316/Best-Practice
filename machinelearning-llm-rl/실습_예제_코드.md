# ì‹¤ìŠµ ì˜ˆì œ ì½”ë“œ: ScaleRL ê¸°ë°˜ì˜ RL í›ˆë ¨

## ğŸ“ ìê¸°ì§„ë‹¨ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ìˆ˜í•™ì  ê°œë… ì´í•´ë„
- [ ] ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì˜ ê¸°ë³¸ í˜•íƒœë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤
- [ ] ì ê·¼ ì„±ëŠ¥ê³¼ ì»´í“¨íŒ… íš¨ìœ¨ì˜ ì°¨ì´ë¥¼ êµ¬ë¶„í•  ìˆ˜ ìˆë‹¤
- [ ] íŒŒë¼ë¯¸í„° ë³€í™”ê°€ ì„±ëŠ¥ ê³¡ì„ ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤

### ê¸°ìˆ  ìš©ì–´ ì´í•´ë„
- [ ] GRPO, DAPO, CISPOì˜ í•µì‹¬ ì°¨ì´ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤
- [ ] PipelineRLê³¼ PPO-off-policyì˜ ì¥ë‹¨ì ì„ ë¹„êµí•  ìˆ˜ ìˆë‹¤
- [ ] FSDPì™€ ì¼ë°˜ ë°ì´í„° ë³‘ë ¬ì˜ ì°¨ì´ë¥¼ ì´í•´í•œë‹¤

### ì‹¤ì œ ì ìš© ëŠ¥ë ¥
- [ ] ì£¼ì–´ì§„ ë°ì´í„°ë¡œ ì‹œê·¸ëª¨ì´ë“œ íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆë‹¤
- [ ] ê°„ë‹¨í•œ RL ì•Œê³ ë¦¬ì¦˜ì˜ í•µì‹¬ ìš”ì†Œë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤
- [ ] ì»´í“¨íŒ… íš¨ìœ¨ì„ ê³ ë ¤í•œ ì‹¤í—˜ ì„¤ê³„ì•ˆì„ ì‘ì„±í•  ìˆ˜ ìˆë‹¤

## 1. ê¸°ë³¸ ì„¤ì •

### 1.1 í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
```python
import torch
import torch.nn as nn
import torch.optim as optim
from transformers import AutoModelForCausalLM, AutoTokenizer
import numpy as np
from typing import List, Dict, Tuple
import matplotlib.pyplot as plt
```

### 1.2 ScaleRL í•µì‹¬ êµ¬ì„± ìš”ì†Œ
```python
class ScaleRLConfig:
    """ScaleRL ì„¤ì • í´ë˜ìŠ¤"""
    
    def __init__(self):
        # ëª¨ë¸ ì„¤ì •
        self.model_name = "meta-llama/Llama-2-7b-hf"  # ì˜ˆì‹œ ëª¨ë¸
        self.max_length = 16384
        
        # RL ì„¤ì •
        self.batch_size = 768  # 48 í”„ë¡¬í”„íŠ¸ Ã— 16 ìƒì„±ë¬¼
        self.generations_per_prompt = 16
        self.learning_rate = 5e-7
        self.epsilon_max = 4.0  # CISPO í´ë¦¬í•‘
        
        # PipelineRL ì„¤ì •
        self.off_policy_steps = 8
        
        # ë°ì´í„° ì„¤ì •
        self.thinking_tokens = 12288
        self.solution_tokens = 2048
        self.prompt_tokens = 2048
        
        # ì •ë°€ë„ ì„¤ì •
        self.use_fp32_lm_head = True
        self.zero_variance_filtering = True
        self.no_positive_resampling = True
```

## 2. ë°ì´í„° ì „ì²˜ë¦¬

### 2.1 ë°ì´í„°ì…‹ ë¡œë“œ
```python
def load_dataset(dataset_path: str, tokenizer, max_length: int):
    """ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬"""
    # Polaris-53K ë°ì´í„°ì…‹ ë¡œë“œ (ì˜ˆì‹œ)
    dataset = load_from_disk(dataset_path)
    
    processed_data = []
    for item in dataset:
        # í”„ë¡¬í”„íŠ¸ í† í° ì œí•œ
        if len(item['thinking']) > max_length - 1000:
            continue
            
        # ì…ë ¥ê³¼ ìƒê° ì¶”ë¡  ë¶„ë¦¬
        prompt = item['prompt']
        thinking = item['thinking']
        solution = item['solution']
        
        # í† í¬ë‚˜ì´ì¦ˆí™”
        inputs = tokenizer(
            prompt,
            thinking,
            solution,
            max_length=max_length,
            truncation=True,
            padding='max_length',
            return_tensors='pt'
        )
        
        processed_data.append({
            'input_ids': inputs['input_ids'],
            'attention_mask': inputs['attention_mask'],
            'labels': inputs['input_ids'].clone(),  # RLì—ì„œëŠ” ì…ë ¥ì„ ë ˆì´ë¸”ë¡œ ì‚¬ìš©
            'reward': item['reward']  # Â±1 ë³´ìƒ
        })
    
    return processed_data
```

### 2.2 ë°ì´í„° ë¡œë”
```python
from torch.utils.data import DataLoader

def create_dataloader(dataset, batch_size: int, shuffle: bool = True):
    """RL í›ˆë ¨ìš© ë°ì´í„° ë¡œë” ìƒì„±"""
    def collate_fn(batch):
        input_ids = torch.stack([item['input_ids'] for item in batch])
        attention_mask = torch.stack([item['attention_mask'] for item in batch])
        rewards = torch.tensor([item['reward'] for item in batch])
        
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'rewards': rewards
        }
    
    return DataLoader(
        dataset, 
        batch_size=batch_size,
        shuffle=shuffle,
        collate_fn=collate_fn
    )
```

## 3. ScaleRL ëª¨ë¸ êµ¬í˜„

### 3.1 CISPO ì†ì‹¤ í•¨ìˆ˜
```python
def compute_cispo_loss(model, batch, epsilon_max: float = 4.0):
    """CISPO ì†ì‹¤ ê³„ì‚°"""
    device = next(model.parameters()).device
    
    # ë°°ì¹˜ ë°ì´í„° ì¶”ì¶œ
    input_ids = batch['input_ids'].to(device)
    attention_mask = batch['attention_mask'].to(device)
    rewards = batch['rewards'].to(device)
    
    # ëª¨ë¸ ì¶œë ¥
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        
    # ì–´ë“œë°´í‹°ì§€ ì •ê·œí™”ëœ ì–´ë“œë°´í‹°ì§€ ê³„ì‚°
    advantages = compute_advantages(rewards)
    advantages_norm = advantages / (advantages.std() + 1e-8)
    
    # ì¤‘ìš”ì„± ê¸¸ì´ ê³„ì‚°
    seq_len = input_ids.shape[1]
    
    # CISPO ì†ì‹¤ ê³„ì‚°
    loss = 0.0
    total_tokens = 0
    
    for i in range(input_ids.shape[0]):  # í”„ë¡¬í”„íŠ¸ë³„
        for j in range(seq_len):  # í† í°ë³„
            # ì¤‘ìš”ì„± ë¹„ìœ¨ ê³„ì‚° (ìƒê° ì¶”ë¡  + í•´ë‹µ)
            thinking_ratio = 12288 / (12288 + 2048 + 2048)
            solution_ratio = (2048 + 2048) / (12288 + 2048 + 2048)
            
            if j < 12288:  # ìƒê° í† í°
                token_ratio = thinking_ratio / seq_len
            elif j < 12288 + 2048:  # í•´ë‹µ í† í°
                token_ratio = solution_ratio / seq_len
            else:  # í”„ë¡¬í”„íŠ¸ í† í°
                token_ratio = 1.0 / seq_len
            
            # ì¤‘ìš”ì„± í† í° ê°€ì¤‘ì¹˜
            importance_weight = min(1.0, epsilon_max)  # ì¤‘ìš”ì„± ìƒ˜í”Œë§
            
            # ë¡œê·¸ í™•ë¥  ê³„ì‚°
            log_probs = torch.log_softmax(logits[i, j, :])
            target_log_probs = log_probs[i, j, input_ids[i, j]]
            
            # ì¤‘ìš”ì„± í•„í„°ë§
            if rewards[i] > 0:  # ì •ë‹µì¸ ê²½ìš°ì—ë§Œ í•™ìŠµ
                token_loss = -importance_weight * target_log_probs
                total_tokens += importance_weight
            else:
                continue  # ì˜¤ë‹µì€ ê±´ë„ˆëœ
            
            loss += token_loss
    
    # í‰ê·  ì†ì‹¤ë¡œ ì •ê·œí™”
    if total_tokens > 0:
        loss = loss / total_tokens
    
    return loss
```

### 3.2 ì–´ë“œë°´í‹°ì§€ ì •ê·œí™”
```python
def compute_advantages(rewards: torch.Tensor) -> torch.Tensor:
    """ë°°ì¹˜ ìˆ˜ì¤€ ì–´ë“œë°´í‹°ì§€ ì •ê·œí™”"""
    # í‰ê·  ë³´ìƒ ê³„ì‚°
    mean_reward = rewards.mean()
    
    # í‘œì¤€í¸ì°¨ ê³„ì‚°
    advantages = rewards - mean_reward
    
    # ë°°ì¹˜ ìˆ˜ì¤€ ì •ê·œí™”
    std_advantages = advantages.std()
    if std_advantages > 1e-8:
        advantages_norm = advantages / std_advantages
    else:
        advantages_norm = advantages
    
    return advantages_norm
```

### 3.3 ì „ì²´ ScaleRL ëª¨ë¸
```python
class ScaleRLModel(nn.Module):
    """ScaleRL ëª¨ë¸ êµ¬í˜„"""
    
    def __init__(self, config: ScaleRLConfig):
        super().__init__()
        self.config = config
        
        # LM í—¤ë“œ FP32 ì •ë°€ë„ ì„¤ì •
        self.base_model = AutoModelForCausalLM.from_pretrained(
            config.model_name,
            torch_dtype=torch.float32 if config.use_fp32_lm_head else torch.float16
        )
        
        # FP32 LM í—¤ë“œ ì„¤ì •
        if config.use_fp32_lm_head:
            self.base_model.lm_head.weight.data = self.base_model.lm_head.weight.data.float()
            self.base_model.lm_head.bias.data = self.base_model.lm_head.bias.data.float() if self.base_model.lm_head.bias is not None else None
    
    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor):
        """ëª¨ë¸ ìˆœì „ ì „íŒŒ"""
        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)
        return outputs
```

## 4. í›ˆë ¨ ë£¨í”„

### 4.1 ê¸°ë³¸ í›ˆë ¨ ë£¨í”„
```python
def train_scale_rl(config: ScaleRLConfig, num_epochs: int = 10):
    """ScaleRL í›ˆë ¨ ë£¨í”„"""
    
    # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
    model = ScaleRLModel(config)
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    train_dataset = load_dataset("polaris-53k.json", tokenizer, config.max_length)
    train_loader = create_dataloader(train_dataset, config.batch_size)
    
    # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
    optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=0.01)
    
    # í›ˆë ¨ ë£¨í”„
    model.train()
    for epoch in range(num_epochs):
        total_loss = 0.0
        num_batches = 0
        
        for batch in train_loader:
            optimizer.zero_grad()
            
            # CISPO ì†ì‹¤ ê³„ì‚°
            loss = compute_cispo_loss(model, batch, config.epsilon_max)
            
            # ì—­ì „íŒŒ
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            num_batches += 1
        
        # ì—í­ë³„ ì†ì‹¤ ì¶œë ¥
        avg_loss = total_loss / num_batches
        print(f"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}")
```

### 4.2 í‰ê°€ ë£¨í”„
```python
def evaluate_model(model, tokenizer, eval_dataset: str, batch_size: int = 32):
    """ëª¨ë¸ í‰ê°€ ë£¨í”„"""
    model.eval()
    
    # í‰ê°€ ë°ì´í„° ë¡œë“œ
    eval_data = load_dataset(eval_dataset, tokenizer, config.max_length)
    eval_loader = create_dataloader(eval_data, batch_size, shuffle=False)
    
    total_correct = 0
    total_samples = 0
    
    with torch.no_grad():
        for batch in eval_loader:
            input_ids = batch['input_ids'].to(next(model.parameters()).device)
            attention_mask = batch['attention_mask'].to(next(model.parameters()).device)
            rewards = batch['rewards'].to(next(model.parameters()).device)
            
            # ëª¨ë¸ ì˜ˆì¸¡
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            
            # ì •ë‹µ ì—¬ë¶€ íŒì • (ë³´ìƒ > 0)
            predictions = (rewards > 0).float()
            
            total_correct += (predictions == rewards).sum().item()
            total_samples += predictions.shape[0]
    
    # í†µê³¼ìœ¨ ê³„ì‚°
    accuracy = total_correct / total_samples
    print(f"Evaluation Accuracy: {accuracy:.4f}")
    
    return accuracy
```

## 5. ì»´í“¨íŒ… íš¨ìœ¨ ìµœì í™”

### 5.1 ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •
```python
def dynamic_batch_sizing(current_performance: float, target_efficiency: float, 
                          current_batch_size: int, min_batch: int = 256, 
                          max_batch: int = 2048):
    """ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •"""
    
    # í˜„ì¬ íš¨ìœ¨ì„±ì´ ëª©í‘œë³´ë‹¤ ë‚®ì€ ê²½ìš° ë°°ì¹˜ ì¦ê°€
    if current_performance < target_efficiency:
        new_batch_size = min(current_batch_size * 1.2, max_batch)
        print(f"Increasing batch size from {current_batch_size} to {new_batch_size}")
        return new_batch_size
    
    # í˜„ì¬ íš¨ìœ¨ì„±ì´ ëª©í‘œë³´ë‹¤ ë†’ì€ ê²½ìš° ë°°ì¹˜ ê°ì†Œ
    elif current_performance > target_efficiency * 1.1:
        new_batch_size = max(min_batch, int(current_batch_size * 0.8))
        print(f"Decreasing batch size from {current_batch_size} to {new_batch_size}")
        return new_batch_size
    
    return current_batch_size
```

### 5.2 í•™ìŠµë¥  ì¡°ì •
```python
def adaptive_learning_rate(current_loss: float, patience: int = 5, 
                       factor: float = 0.5, min_lr: float = 1e-8):
    """ì ì‘í˜• í•™ìŠµë¥  ì¡°ì •"""
    
    # ì†ì‹¤ì´ ê°ì†Œí•˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµë¥  ìœ ì§€
    if current_loss > best_loss * 0.99:
        return current_lr
    
    # ì†ì‹¤ì´ ì¦ê°€í•˜ë©´ í•™ìŠµë¥  ê°ì†Œ
    new_lr = max(current_lr * factor, min_lr)
    print(f"Reducing learning rate from {current_lr} to {new_lr}")
    return new_lr
```

## 6. ë ˆë²¨ 2: ì•Œê³ ë¦¬ì¦˜ ì´í•´ ì‹¤ìŠµ

### 6.1 GRPO ì•Œê³ ë¦¬ì¦˜ í•µì‹¬ ìš”ì†Œ êµ¬í˜„
```python
# ë ˆë²¨ 2: GRPO ì•Œê³ ë¦¬ì¦˜ í•µì‹¬ ìš”ì†Œ êµ¬í˜„
def grpo_basic_implementation():
    """GRPO ì•Œê³ ë¦¬ì¦˜ì˜ í•µì‹¬ ê°œë… êµ¬í˜„"""
    
    # ë¬¸ì œ 1: ê·¸ë£¹ ë‚´ ì–´ë“œë°´í‹°ì§€ ê³„ì‚°
    def calculate_group_advantage(rewards):
        """ë™ì¼ í”„ë¡¬í”„íŠ¸ì—ì„œ ìƒì„±ëœ ë‹µë³€ë“¤ì˜ ìƒëŒ€ì  í‰ê°€"""
        mean_reward = sum(rewards) / len(rewards)
        advantages = [r - mean_reward for r in rewards]
        return advantages
    
    # ë¬¸ì œ 2: ì¤‘ìš”ë„ ìƒ˜í”Œë§ ë¹„ìœ¨ ê³„ì‚°
    def importance_sampling_ratio(old_prob, new_prob, epsilon=4.0):
        """ì •ì±… ë³€í™”ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ ì¡°ì •"""
        ratio = new_prob / old_prob
        return min(ratio, epsilon)  # í´ë¦¬í•‘ ì ìš©
    
    # ì‹¤ì œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
    rewards = [1, -1, 1, -1]  # 4ê°œ ìƒì„±ë¬¼ì˜ ë³´ìƒ
    advantages = calculate_group_advantage(rewards)
    print(f"ì–´ë“œë°´í‹°ì§€: {advantages}")
```

### 6.2 ì „ì²´ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸
```python
def main():
    """ScaleRL í›ˆë ¨ ì‹¤í–‰"""
    
    # ì„¤ì • ì´ˆê¸°í™”
    config = ScaleRLConfig()
    
    # ëª¨ë¸ê³¼ ë°ì´í„° ì¤€ë¹„
    model = ScaleRLModel(config)
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    
    # ë°ì´í„° ë¡œë“œ
    train_dataset = load_dataset("train_data.json", tokenizer, config.max_length)
    eval_dataset = load_dataset("eval_data.json", tokenizer, config.max_length)
    
    train_loader = create_dataloader(train_dataset, config.batch_size)
    eval_loader = create_dataloader(eval_dataset, batch_size=64)
    
    # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
    optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=3, verbose=True
    )
    
    # í›ˆë ¨ ë£¨í”„
    best_accuracy = 0.0
    patience_counter = 0
    
    for epoch in range(50):  # 50 ì—í­
        model.train()
        epoch_loss = 0.0
        
        for batch in train_loader:
            optimizer.zero_grad()
            
            # Zero-variance í•„í„°ë§ ì ìš©
            rewards = batch['rewards']
            non_zero_mask = (rewards != rewards.mean()).float()
            
            if non_zero_mask.sum() > 0:  # ì˜í–¥ë ¥ ìˆëŠ” ë°°ì¹˜ë§Œ ì²˜ë¦¬
                # CISPO ì†ì‹¤ ê³„ì‚°
                loss = compute_cispo_loss(model, batch, config.epsilon_max)
                
                # ì—­ì „íŒŒ
                loss.backward()
                optimizer.step()
                
                epoch_loss += loss.item()
        
        # í•™ìŠµë¥  ì¡°ì •
        scheduler.step(epoch_loss / len(train_loader))
        
        # í‰ê°€
        eval_accuracy = evaluate_model(model, tokenizer, "eval_data.json")
        
        # ìµœì  ëª¨ë¸ ì €ì¥
        if eval_accuracy > best_accuracy:
            best_accuracy = eval_accuracy
            torch.save(model.state_dict(), "best_scale_rl_model.pt")
            patience_counter = 0
        else:
            patience_counter += 1
            
        print(f"Epoch {epoch+1}, Loss: {epoch_loss/len(train_loader):.4f}, "
              f"Eval Accuracy: {eval_accuracy:.4f}, Best: {best_accuracy:.4f}")
        
        # ì¡°ê¸° ì¢…ë£Œ
        if patience_counter >= 10:
            print("Early stopping triggered")
            break

if __name__ == "__main__":
    main()
```

## 7. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

### 7.1 ì»´í“¨íŒ… ê³¡ì„  ì‹œê°í™”
```python
def plot_training_curves(log_file: str):
    """í›ˆë ¨ ê³¡ì„  ì‹œê°í™”"""
    import pandas as pd
    
    # ë¡œê·¸ ë°ì´í„° ë¡œë“œ
    df = pd.read_csv(log_file)
    
    # ì‹œê·¸ëª¨ì´ë“œ ê³¡ì„  í”¼íŒ…
    def sigmoid_curve(x, a, b, c_mid):
        return a / (1 + (c_mid/x)**b)
    
    # ì‹¤ì œ ë°ì´í„°ì— ê³¡ì„  í”¼íŒ…
    from scipy.optimize import curve_fit
    
    x_data = df['gpu_hours'].values
    y_data = df['accuracy'].values
    
    # ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¡œ í”¼íŒ…
    popt, _ = curve_fit(
        lambda x, params: sigmoid_curve(x, params[0], params[1], params[2]),
        x_data, y_data,
        p0=[0.6, 2.0, 5000],  # ì´ˆê¸° ì¶”ì •ì¹˜
        bounds=([0, 1], [0.1, 10], [100, 100000])
    )
    
    a_fit, b_fit, c_mid_fit = popt.x
    
    # í”¼íŒ…ëœ ê³¡ì„  ìƒì„±
    x_fit = np.linspace(min(x_data), max(x_data), 1000)
    y_fit = sigmoid_curve(x_fit, a_fit, b_fit, c_mid_fit)
    
    # ì‹œê°í™”
    plt.figure(figsize=(10, 6))
    plt.scatter(x_data, y_data, alpha=0.6, s=10, label='Actual Data')
    plt.plot(x_fit, y_fit, 'r-', linewidth=2, label=f'Fitted Curve (A={a_fit:.3f}, B={b_fit:.2f})')
    
    plt.xlabel('GPU Hours')
    plt.ylabel('Accuracy')
    plt.title('ScaleRL Training Curve')
    plt.legend()
    plt.grid(True)
    plt.show()
    
    return a_fit, b_fit, c_mid_fit
```

## 8. ì‹¤ìŠµ íŒ

### 8.1 í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­
- **GPU**: Nvidia A100 ì´ìƒ, VRAM ìµœì†Œ 40GB
- **ë©”ëª¨ë¦¬**: ëª¨ë¸ í¬ê¸°ì˜ 2-4ë°°, ìµœì†Œ 80GB ì‹œìŠ¤í…œ RAM
- **ìŠ¤í† ë¦¬ì§€**: ìµœì†Œ 1TBì˜ ê³ ì† ìŠ¤í† ë¦¬ì§€
- **ì†Œí”„íŠ¸ì›¨ì–´**: Python 3.8+, PyTorch 2.0+, Transformers 4.30+

### 8.2 ì‹¤í–‰ ìˆœì„œ
```bash
# 1. í™˜ê²½ ì„¤ì •
conda create -n scale_rl python=3.9
conda activate scale_rl

# 2. ë°ì´í„° ì¤€ë¹„
python prepare_data.py --input polaris_raw --output polaris_processed

# 3. í›ˆë ¨ ì‹¤í–‰
python train_scale_rl.py \
    --config config.yaml \
    --train-data polaris_processed/train.json \
    --eval-data polaris_processed/eval.json \
    --output-dir ./outputs \
    --log-file training_log.csv

# 4. ì„±ëŠ¥ í‰ê°€
python evaluate_model.py \
    --model-path ./outputs/best_scale_rl_model.pt \
    --eval-data polaris_processed/test.json \
    --output-file evaluation_results.json
```

### 8.3 ë¬¸ì œ í•´ê²° ê°€ì´ë“œ
| ë¬¸ì œ ìœ í˜• | ì¼ë°˜ì ì¸ í•´ê²°ì±… | ScaleRL íŠ¹í™” í•´ê²°ì±… |
|----------|----------------|----------------|----------------|
| **ìˆ˜ì¹˜ì  ë¶ˆì•ˆì •** | í•™ìŠµë¥  ê°ì†Œ, ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ | FP32 ì •ë°€ë„, ì•ˆì •ì  ë°°ì¹˜ í¬ê¸° |
| **ì„±ëŠ¥ í–¥ìƒ ì •ì²´** | ì´ˆê¸°ì—ëŠ” ë¹ ë¥´ë‚˜ í›„ê¸°ì—ëŠ” ì •ì²´ | ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •, í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ |
| **ë©”ëª¨ë¦¬ ë¶€ì¡±** | OOM ì—ëŸ¬ ë°œìƒ | ê·¸ë˜ë””ì–¸íŠ¸ ì¶•ì , FP16 í˜¼í•© ì •ë°€ë„ |
| **ì»´í“¨íŒ… íš¨ìœ¨ ì €í•˜** | GPU í™œìš©ë¥  70% ë¯¸ë§Œ | ë°ì´í„° íŒŒì´í”„ë¼ì¸ ìµœì í™”, PipelineRL ìµœì í™” |

ì´ ì½”ë“œ ì˜ˆì‹œë“¤ì€ ScaleRLì˜ í•µì‹¬ ì›ë¦¬ë¥¼ êµ¬í˜„í•˜ë©°, ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œ ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ì‹¤ìš©ì ì¸ ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.