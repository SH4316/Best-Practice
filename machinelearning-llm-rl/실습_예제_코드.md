# 실습 예제 코드: ScaleRL 기반의 RL 훈련

## 1. 기본 설정

### 1.1 필요한 라이브러리
```python
import torch
import torch.nn as nn
import torch.optim as optim
from transformers import AutoModelForCausalLM, AutoTokenizer
import numpy as np
from typing import List, Dict, Tuple
import matplotlib.pyplot as plt
```

### 1.2 ScaleRL 핵심 구성 요소
```python
class ScaleRLConfig:
    """ScaleRL 설정 클래스"""
    
    def __init__(self):
        # 모델 설정
        self.model_name = "meta-llama/Llama-2-7b-hf"  # 예시 모델
        self.max_length = 16384
        
        # RL 설정
        self.batch_size = 768  # 48 프롬프트 × 16 생성물
        self.generations_per_prompt = 16
        self.learning_rate = 5e-7
        self.epsilon_max = 4.0  # CISPO 클리핑
        
        # PipelineRL 설정
        self.off_policy_steps = 8
        
        # 데이터 설정
        self.thinking_tokens = 12288
        self.solution_tokens = 2048
        self.prompt_tokens = 2048
        
        # 정밀도 설정
        self.use_fp32_lm_head = True
        self.zero_variance_filtering = True
        self.no_positive_resampling = True
```

## 2. 데이터 전처리

### 2.1 데이터셋 로드
```python
def load_dataset(dataset_path: str, tokenizer, max_length: int):
    """데이터셋 로드 및 전처리"""
    # Polaris-53K 데이터셋 로드 (예시)
    dataset = load_from_disk(dataset_path)
    
    processed_data = []
    for item in dataset:
        # 프롬프트 토큰 제한
        if len(item['thinking']) > max_length - 1000:
            continue
            
        # 입력과 생각 추론 분리
        prompt = item['prompt']
        thinking = item['thinking']
        solution = item['solution']
        
        # 토크나이즈화
        inputs = tokenizer(
            prompt,
            thinking,
            solution,
            max_length=max_length,
            truncation=True,
            padding='max_length',
            return_tensors='pt'
        )
        
        processed_data.append({
            'input_ids': inputs['input_ids'],
            'attention_mask': inputs['attention_mask'],
            'labels': inputs['input_ids'].clone(),  # RL에서는 입력을 레이블로 사용
            'reward': item['reward']  # ±1 보상
        })
    
    return processed_data
```

### 2.2 데이터 로더
```python
from torch.utils.data import DataLoader

def create_dataloader(dataset, batch_size: int, shuffle: bool = True):
    """RL 훈련용 데이터 로더 생성"""
    def collate_fn(batch):
        input_ids = torch.stack([item['input_ids'] for item in batch])
        attention_mask = torch.stack([item['attention_mask'] for item in batch])
        rewards = torch.tensor([item['reward'] for item in batch])
        
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'rewards': rewards
        }
    
    return DataLoader(
        dataset, 
        batch_size=batch_size,
        shuffle=shuffle,
        collate_fn=collate_fn
    )
```

## 3. ScaleRL 모델 구현

### 3.1 CISPO 손실 함수
```python
def compute_cispo_loss(model, batch, epsilon_max: float = 4.0):
    """CISPO 손실 계산"""
    device = next(model.parameters()).device
    
    # 배치 데이터 추출
    input_ids = batch['input_ids'].to(device)
    attention_mask = batch['attention_mask'].to(device)
    rewards = batch['rewards'].to(device)
    
    # 모델 출력
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        
    # 어드밴티지 정규화된 어드밴티지 계산
    advantages = compute_advantages(rewards)
    advantages_norm = advantages / (advantages.std() + 1e-8)
    
    # 중요성 길이 계산
    seq_len = input_ids.shape[1]
    
    # CISPO 손실 계산
    loss = 0.0
    total_tokens = 0
    
    for i in range(input_ids.shape[0]):  # 프롬프트별
        for j in range(seq_len):  # 토큰별
            # 중요성 비율 계산 (생각 추론 + 해답)
            thinking_ratio = 12288 / (12288 + 2048 + 2048)
            solution_ratio = (2048 + 2048) / (12288 + 2048 + 2048)
            
            if j < 12288:  # 생각 토큰
                token_ratio = thinking_ratio / seq_len
            elif j < 12288 + 2048:  # 해답 토큰
                token_ratio = solution_ratio / seq_len
            else:  # 프롬프트 토큰
                token_ratio = 1.0 / seq_len
            
            # 중요성 토큰 가중치
            importance_weight = min(1.0, epsilon_max)  # 중요성 샘플링
            
            # 로그 확률 계산
            log_probs = torch.log_softmax(logits[i, j, :])
            target_log_probs = log_probs[i, j, input_ids[i, j]]
            
            # 중요성 필터링
            if rewards[i] > 0:  # 정답인 경우에만 학습
                token_loss = -importance_weight * target_log_probs
                total_tokens += importance_weight
            else:
                continue  # 오답은 건너뜁
            
            loss += token_loss
    
    # 평균 손실로 정규화
    if total_tokens > 0:
        loss = loss / total_tokens
    
    return loss
```

### 3.2 어드밴티지 정규화
```python
def compute_advantages(rewards: torch.Tensor) -> torch.Tensor:
    """배치 수준 어드밴티지 정규화"""
    # 평균 보상 계산
    mean_reward = rewards.mean()
    
    # 표준편차 계산
    advantages = rewards - mean_reward
    
    # 배치 수준 정규화
    std_advantages = advantages.std()
    if std_advantages > 1e-8:
        advantages_norm = advantages / std_advantages
    else:
        advantages_norm = advantages
    
    return advantages_norm
```

### 3.3 전체 ScaleRL 모델
```python
class ScaleRLModel(nn.Module):
    """ScaleRL 모델 구현"""
    
    def __init__(self, config: ScaleRLConfig):
        super().__init__()
        self.config = config
        
        # LM 헤드 FP32 정밀도 설정
        self.base_model = AutoModelForCausalLM.from_pretrained(
            config.model_name,
            torch_dtype=torch.float32 if config.use_fp32_lm_head else torch.float16
        )
        
        # FP32 LM 헤드 설정
        if config.use_fp32_lm_head:
            self.base_model.lm_head.weight.data = self.base_model.lm_head.weight.data.float()
            self.base_model.lm_head.bias.data = self.base_model.lm_head.bias.data.float() if self.base_model.lm_head.bias is not None else None
    
    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor):
        """모델 순전 전파"""
        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)
        return outputs
```

## 4. 훈련 루프

### 4.1 기본 훈련 루프
```python
def train_scale_rl(config: ScaleRLConfig, num_epochs: int = 10):
    """ScaleRL 훈련 루프"""
    
    # 모델과 토크나이저 로드
    model = ScaleRLModel(config)
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    
    # 데이터셋 로드
    train_dataset = load_dataset("polaris-53k.json", tokenizer, config.max_length)
    train_loader = create_dataloader(train_dataset, config.batch_size)
    
    # 옵티마이저 설정
    optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=0.01)
    
    # 훈련 루프
    model.train()
    for epoch in range(num_epochs):
        total_loss = 0.0
        num_batches = 0
        
        for batch in train_loader:
            optimizer.zero_grad()
            
            # CISPO 손실 계산
            loss = compute_cispo_loss(model, batch, config.epsilon_max)
            
            # 역전파
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            num_batches += 1
        
        # 에폭별 손실 출력
        avg_loss = total_loss / num_batches
        print(f"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}")
```

### 4.2 평가 루프
```python
def evaluate_model(model, tokenizer, eval_dataset: str, batch_size: int = 32):
    """모델 평가 루프"""
    model.eval()
    
    # 평가 데이터 로드
    eval_data = load_dataset(eval_dataset, tokenizer, config.max_length)
    eval_loader = create_dataloader(eval_data, batch_size, shuffle=False)
    
    total_correct = 0
    total_samples = 0
    
    with torch.no_grad():
        for batch in eval_loader:
            input_ids = batch['input_ids'].to(next(model.parameters()).device)
            attention_mask = batch['attention_mask'].to(next(model.parameters()).device)
            rewards = batch['rewards'].to(next(model.parameters()).device)
            
            # 모델 예측
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            
            # 정답 여부 판정 (보상 > 0)
            predictions = (rewards > 0).float()
            
            total_correct += (predictions == rewards).sum().item()
            total_samples += predictions.shape[0]
    
    # 통과율 계산
    accuracy = total_correct / total_samples
    print(f"Evaluation Accuracy: {accuracy:.4f}")
    
    return accuracy
```

## 5. 컴퓨팅 효율 최적화

### 5.1 동적 배치 크기 조정
```python
def dynamic_batch_sizing(current_performance: float, target_efficiency: float, 
                          current_batch_size: int, min_batch: int = 256, 
                          max_batch: int = 2048):
    """동적 배치 크기 조정"""
    
    # 현재 효율성이 목표보다 낮은 경우 배치 증가
    if current_performance < target_efficiency:
        new_batch_size = min(current_batch_size * 1.2, max_batch)
        print(f"Increasing batch size from {current_batch_size} to {new_batch_size}")
        return new_batch_size
    
    # 현재 효율성이 목표보다 높은 경우 배치 감소
    elif current_performance > target_efficiency * 1.1:
        new_batch_size = max(min_batch, int(current_batch_size * 0.8))
        print(f"Decreasing batch size from {current_batch_size} to {new_batch_size}")
        return new_batch_size
    
    return current_batch_size
```

### 5.2 학습률 조정
```python
def adaptive_learning_rate(current_loss: float, patience: int = 5, 
                       factor: float = 0.5, min_lr: float = 1e-8):
    """적응형 학습률 조정"""
    
    # 손실이 감소하지 않으면 학습률 유지
    if current_loss > best_loss * 0.99:
        return current_lr
    
    # 손실이 증가하면 학습률 감소
    new_lr = max(current_lr * factor, min_lr)
    print(f"Reducing learning rate from {current_lr} to {new_lr}")
    return new_lr
```

## 6. 실제 적용 예시

### 6.1 전체 훈련 스크립트
```python
def main():
    """ScaleRL 훈련 실행"""
    
    # 설정 초기화
    config = ScaleRLConfig()
    
    # 모델과 데이터 준비
    model = ScaleRLModel(config)
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    
    # 데이터 로드
    train_dataset = load_dataset("train_data.json", tokenizer, config.max_length)
    eval_dataset = load_dataset("eval_data.json", tokenizer, config.max_length)
    
    train_loader = create_dataloader(train_dataset, config.batch_size)
    eval_loader = create_dataloader(eval_dataset, batch_size=64)
    
    # 옵티마이저 설정
    optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=3, verbose=True
    )
    
    # 훈련 루프
    best_accuracy = 0.0
    patience_counter = 0
    
    for epoch in range(50):  # 50 에폭
        model.train()
        epoch_loss = 0.0
        
        for batch in train_loader:
            optimizer.zero_grad()
            
            # Zero-variance 필터링 적용
            rewards = batch['rewards']
            non_zero_mask = (rewards != rewards.mean()).float()
            
            if non_zero_mask.sum() > 0:  # 영향력 있는 배치만 처리
                # CISPO 손실 계산
                loss = compute_cispo_loss(model, batch, config.epsilon_max)
                
                # 역전파
                loss.backward()
                optimizer.step()
                
                epoch_loss += loss.item()
        
        # 학습률 조정
        scheduler.step(epoch_loss / len(train_loader))
        
        # 평가
        eval_accuracy = evaluate_model(model, tokenizer, "eval_data.json")
        
        # 최적 모델 저장
        if eval_accuracy > best_accuracy:
            best_accuracy = eval_accuracy
            torch.save(model.state_dict(), "best_scale_rl_model.pt")
            patience_counter = 0
        else:
            patience_counter += 1
            
        print(f"Epoch {epoch+1}, Loss: {epoch_loss/len(train_loader):.4f}, "
              f"Eval Accuracy: {eval_accuracy:.4f}, Best: {best_accuracy:.4f}")
        
        # 조기 종료
        if patience_counter >= 10:
            print("Early stopping triggered")
            break

if __name__ == "__main__":
    main()
```

## 7. 성능 모니터링

### 7.1 컴퓨팅 곡선 시각화
```python
def plot_training_curves(log_file: str):
    """훈련 곡선 시각화"""
    import pandas as pd
    
    # 로그 데이터 로드
    df = pd.read_csv(log_file)
    
    # 시그모이드 곡선 피팅
    def sigmoid_curve(x, a, b, c_mid):
        return a / (1 + (c_mid/x)**b)
    
    # 실제 데이터에 곡선 피팅
    from scipy.optimize import curve_fit
    
    x_data = df['gpu_hours'].values
    y_data = df['accuracy'].values
    
    # 시그모이드 함수로 피팅
    popt, _ = curve_fit(
        lambda x, params: sigmoid_curve(x, params[0], params[1], params[2]),
        x_data, y_data,
        p0=[0.6, 2.0, 5000],  # 초기 추정치
        bounds=([0, 1], [0.1, 10], [100, 100000])
    )
    
    a_fit, b_fit, c_mid_fit = popt.x
    
    # 피팅된 곡선 생성
    x_fit = np.linspace(min(x_data), max(x_data), 1000)
    y_fit = sigmoid_curve(x_fit, a_fit, b_fit, c_mid_fit)
    
    # 시각화
    plt.figure(figsize=(10, 6))
    plt.scatter(x_data, y_data, alpha=0.6, s=10, label='Actual Data')
    plt.plot(x_fit, y_fit, 'r-', linewidth=2, label=f'Fitted Curve (A={a_fit:.3f}, B={b_fit:.2f})')
    
    plt.xlabel('GPU Hours')
    plt.ylabel('Accuracy')
    plt.title('ScaleRL Training Curve')
    plt.legend()
    plt.grid(True)
    plt.show()
    
    return a_fit, b_fit, c_mid_fit
```

## 8. 실습 팁

### 8.1 하드웨어 요구사항
- **GPU**: Nvidia A100 이상, VRAM 최소 40GB
- **메모리**: 모델 크기의 2-4배, 최소 80GB 시스템 RAM
- **스토리지**: 최소 1TB의 고속 스토리지
- **소프트웨어**: Python 3.8+, PyTorch 2.0+, Transformers 4.30+

### 8.2 실행 순서
```bash
# 1. 환경 설정
conda create -n scale_rl python=3.9
conda activate scale_rl

# 2. 데이터 준비
python prepare_data.py --input polaris_raw --output polaris_processed

# 3. 훈련 실행
python train_scale_rl.py \
    --config config.yaml \
    --train-data polaris_processed/train.json \
    --eval-data polaris_processed/eval.json \
    --output-dir ./outputs \
    --log-file training_log.csv

# 4. 성능 평가
python evaluate_model.py \
    --model-path ./outputs/best_scale_rl_model.pt \
    --eval-data polaris_processed/test.json \
    --output-file evaluation_results.json
```

### 8.3 문제 해결 가이드
| 문제 유형 | 일반적인 해결책 | ScaleRL 특화 해결책 |
|----------|----------------|----------------|----------------|
| **수치적 불안정** | 학습률 감소, 그래디언트 클리핑 | FP32 정밀도, 안정적 배치 크기 |
| **성능 향상 정체** | 초기에는 빠르나 후기에는 정체 | 동적 배치 크기 조정, 학습률 스케줄링 |
| **메모리 부족** | OOM 에러 발생 | 그래디언트 축적, FP16 혼합 정밀도 |
| **컴퓨팅 효율 저하** | GPU 활용률 70% 미만 | 데이터 파이프라인 최적화, PipelineRL 최적화 |

이 코드 예시들은 ScaleRL의 핵심 원리를 구현하며, 실제 프로젝트에서 즉시 적용 가능한 실용적인 가이드를 제공합니다.