# 제4부: 주요 실험 결과 및 발견사항 분석

## 1. 핵심 발견 1: 성능 한계의 보편성 부재

### 1.1 서로 다른 RL 방법들의 점근 성능 차이
$$
\begin{array}{c|c|c|c}
\text{방법} & \text{점근 성능}(A) & \text{컴퓨팅 효율}(B) \\
\hline
\text{DeepSeek (GRPO)} & 0.48 & 1.45 \\
\text{Qwen2.5 (DAPO)} & 0.52 & 1.62 \\
\text{Magistral} & 0.54 & 1.68 \\
\text{MiniMax} & 0.58 & 1.85 \\
\text{ScaleRL} & \mathbf{0.61} & \mathbf{2.01} \\
\end{array}
$$

### 1.2 성능 한계 결정 요인
- **손실 함수 타입**: CISPO/GSPO > DAPO/GRPO
- **모델 정밀도**: FP32 > FP16 (LM head)
- **배치 크기**: 더 큰 배치가 더 높은 점근 성능

### 1.3 "쓴디 레슨"의 중요성 재조명
> 작은 컴퓨팅에서 우세했던 방법이 대규모에서 열세하는 현상 발견

## 2. 핵심 발견 2: 효율성 vs 점근 성능

### 2.1 효율성과 점근 성능의 상추 관계
$$
\text{효율성}(B) \times \text{점근 성능}(A) = \text{최종 성능 향상}
$$

### 2.2 설계 선택의 효과 분석
| 설계 축 | 주요 영향 | 효율성($B$) | 점근 성능($A$) |
|---------|----------|-----------|-------------|-------------|
| **손실 함수** | CISPO vs DAPO | 높음 | 높음 |
| **정밀도** | FP32 vs FP16 | 높음 | 약간 향상 |
| **배치 크기** | 768 → 2048 | 낮음 | 높음 |
| **정규화** | 배치 vs 프롬프트 | 약간 향상 | 거의 무관 |

### 2.3 "쓴디 레슨"의 경제학적 해석
- **한계 생산성**: 초기 컴퓨팅 효율이 높을수록 후기 향상률 감소
- **최적 전략**: 점근 성능($A$) 최대화 우선, 효율성($B$) 개선은 그 다음

## 3. 핵심 발견 3: 설계 선택의 효과 분석

### 3.1 대부분 설계 선택은 효율성에 영향
- **손실 집계**: 프롬프트 > 토큰 > 샘플 평균
- **어드밴티지 정규화**: 배치 > 프롬프트 > 무정규화
- **데이터 커리큘럼**: No-Positive-Resampling 효율성 향상
- **길이 제어**: 인터럽션 > 페널티

### 3.2 점근 성능에 영향하는 소수 핵심 요인
| 핵심 요인 | 영향 정도 | 설명 |
|----------|----------|------|
| **손실 함수** | 매우 높음 | 알고리즘의 근본적 한계 결정 |
| **FP32 정밀도** | 높음 | 수치적 안정성 확보 |
| **배치 크기** | 높음 | 데이터 다양성과 통계적 안정성 |
| **오피리 알고리즘** | 높음 | 훈련 안정성과 효율성 |

### 3.3 설계 선택의 상대적 중요도 순위
1. **오피리 알고리즘** (PipelineRL vs PPO-off-policy)
2. **손실 함수** (CISPO/GSPO vs DAPO/GRPO)
3. **모델 정밀도** (FP32 vs FP16)
4. **배치 크기** (더 큰 배치)
5. **기타 세부사항** (정규화, 커리큘럼 등)

## 4. 통계적 유의성과 오차 분석

### 4.1 반복 실험의 통계적 유의성
- **독립 실행**: 3회의 독립적인 ScaleRL 실행
- **일관성**: 파라미터 추정치의 안정성 확인
- **오차 범위**: $A \pm 0.02$, $B$의 상대적 비교 가능

### 4.2 통계적 검증 방법
- **F-검증**: 다른 알고리즘 간의 성능 차이 검증
- **T-검증**: 동일 알고리즘 내 다른 설정 간의 성능 차이 검증
- **교차 검증**: Leave-One-Out 분석으로 각 요인의 기여도 확인

### 4.3 신뢰도 구간 설정
- **초기 구간**: 1.5K GPU시간 이전 데이터 제외
- **이유**: 초기 훈련의 불안정성 제거
- **안정 구간**: 1.5K-50K GPU시간에서 안정적 피팅

## 5. 예상치 못한 발견

### 5.1 엔트로피의 역할
- **긍정적 영향**: 높은 엔트로피가 더 나은 성능 향상
- **최적 범위**: 0.3-0.7 사이에서 최적
- **낮은 엔트로피**: 성능 저하 또는 불안정성 유발

### 5.2 생성 길이와 효율성의 관계
- **예상**: 긴 생성 길이가 훈련 효율을 저하
- **실제**: 초기 효율성 저하 but 최종 성능 향상
- **시사점**: 긴 컨텍스트는 성능 한계 상향이 아닌 효율성 트레이드오프

## 6. 강의 핵심 메시지

### 6.1 실무적 함의
> "성능 한계는 알고리즘 선택으로 결정되지만, 효율성은 설계 세부사항으로 결정됩니다."

### 6.2 연구적 함의
> "대규모 RL 훈련의 성공은 올바른 이론과 방법론, 그리고 체계적인 실험 설계에 달려 있습니다."

### 6.3 다음 강의로의 연결
> "이제 구체적인 ScaleRL 구성과 실제 적용 사례를 살펴보겠습니다."