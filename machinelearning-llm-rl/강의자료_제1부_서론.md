# 제1부: 서론 - RL 컴퓨팅의 중요성과 현재 과제

## 1. LLM 발전에서 RL의 역할 변화

### 과거: 사전훈련 중심의 LLM 개발
- 2018-2022: GPT, BERT 등 대규모 언어모델 등장
- 주요 초점: 사전훈련 데이터의 양과 질 확대
- 한계: 지식 암기에 한계, 추론 능력 제약

### 현재: RL의 부상과 새로운 패러다임
- 2023-2025: ChatGPT, Claude 등 대화형 AI 등장
- **RL의 핵심 역할**:
  - 사전훈련으로 얻은 지식을 실제 추론 능력으로 전환
  - 시간추론(test-time thinking) 능력 부여
  - 에이전트(agent) 기능 구현
  - 복잡한 문제 해결 능력 향상

### 컴퓨팅 예산의 폭발적 증가
```
컴퓨팅 예산 (GPU 시간)
DeepSeek-R1-Zero: 100,000 H800 GPU 시간
  - 전체 훈련 컴퓨팅의 3.75%
OpenAI o1 → o3: 10배 이상 증가
Grok-3 → Grok-4: 유사한 증가 폭
```

## 2. 컴퓨팅 예산의 과학적 예측 필요성

### 현재 문제점: "예술"에 머무른 RL 훈련
- **알고리즘 중심의 접근**: DAPO, GRPO 등 개별 알고리즘 연구
- **모델 특정적 보고서**: MiniMax, Magistral 등 특정 모델 훈련 사례
- **임시방안의 한계**: 특정 문맥에만 적용 가능한 임시 해결책

### 학술적 공허의 부재
- 사전훈련: Kaplan et al. (2020), Hoffmann et al. (2022) 등 스케일링 법칙 확립
- RL 훈련: 이와 유사한 예측 프레임워크 부재
- **결과**: 대규모 실험이 없으면 연구 진행 어려움

## 3. 본 연구의 목표와 접근 방법

### 연구 목표
1. **예측 프레임워크 개발**: RL 컴퓨팅-성능 관계의 수학적 모델링
2. **대규모 실험 설계**: 400,000 GPU시간 규모의 체계적 연구
3. **최적 레시피 개발**: ScaleRL이라는 베스트 프랙티스 제안
4. **일반화 가능성 검증**: 다양한 축(모델, 태스크, 데이터)에서의 확장성

### 핵심 가설
- **가설 1**: RL 성능은 시그모이드 형태의 포화 곡선을 따른다
- **가설 2**: 성능 한계(A)는 알고리즘 선택에 따라 달라진다
- **가설 3**: 대부분의 설계 선택은 효율성(B)에 영향을 준다

### 연구 방법론 개요
```
1단계: 기준 알고리즘 설정 (GRPO + DAPO 클리핑)
2단계: 개별 설계 축 소규모 실험 (3.5-4K GPU시간)
3단계: 안정적 조합 발견 시 대규모 실험 (16K GPU시간)
4단계: 최적 레시피 통합 및 100K GPU시간 검증
5단계: 다차원적 확장성 검증 (모델 크기, 태스크 등)
```

## 4. 연구의 기대 효과

### 학술적 기여
- **RL 스케일링 이론 정립**: 최초의 체계적 RL 확장성 연구
- **방법론적 혁신**: 작은 규모 실험으로 대규모 성능 예측
- **새로운 평가 지표**: 점근 성능(A)과 컴퓨팅 효율(B)의 분리

### 산업적 기여
- **컴퓨팅 효율화**: 불필요한 컴퓨팅 낭비 방지
- **안정성 확보**: 대규모 RL 훈련의 안정성 보장
- **예측 가능성**: 컴퓨팅 투자 수익(ROI) 예측

### 사회적 기여
- **연구 민주화**: 대규모 컴퓨팅 없이도 연구 가능
- **기술 접근성**: 검증된 레시피의 공개로 기술 확산
- **지속 가능한 AI**: 효율적 컴퓨팅으로 에너지 소비 최적화

## 5. 강의 핵심 메시지

> "RL 컴퓨팅은 더 이상 예술이 아닌 과학입니다.  
> 올바른 이론과 방법론으로 예측 가능한 확장을 달성할 수 있습니다."

## 6. RL 스케일링 핵심 용어집

### 🎯 성능 관련 용어
| 용어 | 영문 | 정의 및 목적 | RL 훈련에서의 의미 |
|------|------|------------|------------------|
| **점근 성능** | Asymptotic Performance | 무한 컴퓨팅 투자 시 도달 가능한 최대 성능 한계 | 알고리즘이 가진 근본적 성능 상한을 결정하는 지표 |
| **컴퓨팅 효율** | Compute Efficiency | 단위 컴퓨팅 당 성능 향상률을 나타내는 지표 | 동일한 성능에 도달하기 위해 필요한 컴퓨팅 양을 최소화하는 정도 |
| **수확 체감** | Diminishing Returns | 투자 증가에 따른 효과 증가율이 감소하는 현상 | 컴퓨팅 투자가 일정 수준을 넘으면 성능 향상이 둔화되는 현상 |

### 🔧 알고리즘 관련 용어
| 용어 | 영문 | 정의 및 목적 | RL 훈련에서의 역할 |
|------|------|------------|------------------|
| **GRPO** | Group Relative Policy Optimization | 동일 프롬프트에서 생성된 답변 그룹 내 상대적 품질 평가 | 프롬프트 난이도 차이로 인한 편향을 제거하고 안정적 학습 유도 |
| **DAPO** | Direct Advantage Policy Optimization | 어드밴티지를 직접 최적화하는 정책 경향법 | 정책 개선과 다양성 유지 사이의 균형을 맞추는 알고리즘 |
| **CISPO** | Truncated Importance Sampling | 중요도 샘플링의 극단적 값을 제한하는 방법 | 학습 안정성 확보를 위해 과도한 정책 변화를 방지 |

### 🏗️ 시스템 관련 용어
| 용어 | 영문 | 정의 및 목적 | RL 훈련에서의 기능 |
|------|------|------------|------------------|
| **PipelineRL** | Pipeline Reinforcement Learning | 생성과 훈련을 파이프라인 방식으로 연속 처리하는 아키텍처 | 생성기-훈련기 간 지연 최소화로 컴퓨팅 효율 향상 |
| **FSDP** | Fully Sharded Data Parallel | 모델 파라미터를 완전히 분산하여 병렬 처리하는 방식 | 대규모 모델의 메모리 요구량 감소 및 훈련 속도 향상 |
| **오피리즘** | Off-policyness | 현재 학습 정책과 데이터 생성 정책 간의 차이 정도 | 훈련 안정성과 효율성에 영향을 미치는 핵심 파라미터 |

### 다음 강의에서 다룰 내용
- 제2부: 시그모이드 스케일링 법칙의 수학적 기초
- 제3부: 400,000 GPU시간 실험의 상세 설계
- 제4부: 성능 한계와 효율성의 상추 관계
- 제5부: ScaleRL 레시피의 구성과 검증